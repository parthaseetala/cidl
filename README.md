This repo contains the source code for the projects I demonstrate in my CIDL webseries.

CIDL -- Comprehensive and Intuitive Introduction to Deep Learning

## WHY

Wouldn't it be great if there was an intuitive and digestible way to learn both the *depth* and *breadth* of AI? Too often, AI looks like magic from the outside -- but it’s not magic, it's a clever method. Engineers especially need to see behind the curtain. Once we understand *why* and *how* the magic works, we're no longer just admiring the illusion, we're learning the craft, ready to design and perform our own. That's the goal of my CIDL webseries.

## BACKGROUND

Learning AI, however, is not easy. Topics are highly technical and cover a variety of fields without offering an intuitive or clear entry point for someone new to the field. Most tutorials focus on heavy math, making them inaccessible to most people; or treat topics only superficially; or cover concepts in a bespoke way without tying them together clearly. 

This started as an experiment: Can we teach both the **breadth** and **depth** of AI in a way that’s both **intuitive** and **accessible**? AI is a complex, technical field -- but too often, that complexity becomes a barrier. There's a kind of elitism at play, where ideas are presented in ways that feel needlessly complicated and exclusive. My aim was to make AI education more inclusive -- to break down the field in a clear, structured, and comprehensive way. The goal isn’t just to simplify, but to deepen understanding -- helping more people grasp the fundamentals of AI in a meaningful, lasting way -- far beyond a surface-level understanding.

## AGENDA

- **SEASON 1 -- Neural Network Fundamentals**
  - Episode 1: [Intuitive Intro to Neural Networks](https://youtu.be/os5by3jKUvc)
  - Episode 2: [Solving real usecases with Neural Networks](https://youtu.be/CQTCS8SO8bs)
  - Episode 3: [Tuning techniques for Neural Networks](https://youtu.be/_JWcxDN8BkQ)

- **SEASON 2 -- Natural Language Processing (NLP) and Timeseries Forecasting**
  - Episode 1: [Tokenization Techniques](https://youtu.be/Oy48ZLHAUg0)
  - Episode 2: [Word Embedding -- converting text to vectors](https://youtu.be/8jqqE8XG5T0)
  - Episode 3: [RNN -- Recurrent Neural Networks explained simply, intuitively and comprehensively](https://youtu.be/VuzcUsg0GVs)
  - Episode 4: [LSTM -- Long Short-Term Memory explained simply, intuitively and comprehensively](https://youtu.be/IVTZ-v4qURY)
  - Episode 5: [Seq2Seq Networks -- building conversational language interfaces](https://www.youtube.com/watch?v=eTknarEWVm8)

- **SEASON 3 -- Transformers and Large Language Models**
  - Episode 1: Introduction to Transformer Architecture and LLMs -- a holistic overview
  - Episode 2: Encoder-only Transformer explained simply, intuitively and comprehensively
  - Episode 3: Decoder-only Transformer explained simply, intuitively and comprehensively
  - Episode 4: Encoder-Decoder Transformer explained simply, intuitively and comprehensively
  - Episode 5: Optimizing LLMs for speed and performance (KVCaching, PEFT, LoRA, Quantization, Distillation, MTP)
  - Episode 6: Optimizing LLMs for quality (MLA, Sampling Techniques, Temperature, MoE)
  - Episode 7: Aligning LLMs to human preferences (RLHF, PPO, GRPO)
  - Episode 8: Combining Search with Text Generation (RAG, Vector Databases)

## PLAYLIST

**Entire playlist is available here:** [https://www.youtube.com/playlist?list=PLpKnsnE7SJVopIOfWptNwBnbys1coetbK](https://www.youtube.com/playlist?list=PLpKnsnE7SJVopIOfWptNwBnbys1coetbK)
