{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMaDn/vTzB7tPj1tBFzNjYy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CLI: Conversational Language Interface**\n",
        "### **Converting English into Linux commands**\n",
        "\n",
        "**Author: Partha Seetala**\n",
        "\n",
        "**Video Tutorial: https://www.youtube.com/watch?v=eTknarEWVm8**"
      ],
      "metadata": {
        "id": "eICl05PQjCyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import relevant Python modules**"
      ],
      "metadata": {
        "id": "cSk0qUo-_pd9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sfWczGaX6lYa"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import pickle\n",
        "import json\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Input, Embedding, LSTM, Attention, Concatenate, Dense, Masking)\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Helper function to generate training data**"
      ],
      "metadata": {
        "id": "7TaXRA0epjMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_english_to_cli_sentences():\n",
        "\n",
        "    def rand_str(minlen=3, maxlen=6):\n",
        "        val = ''.join(random.choice(string.ascii_uppercase) for _ in range(random.randint(minlen, maxlen)))\n",
        "        return val\n",
        "\n",
        "    pleasantries = [\"\", \"please\", \"kindly\", \"hey\", \"could you\"]\n",
        "    words = [rand_str() for _ in range(5)]\n",
        "    filenames = [rand_str() for _ in range(5)]\n",
        "    dirnames = [rand_str() for _ in range(5)]\n",
        "    oldnames = [rand_str() for _ in range(5)]\n",
        "    newnames = [rand_str() for _ in range(5)]\n",
        "    srcs = [rand_str() for _ in range(5)]\n",
        "    dests = [rand_str() for _ in range(5)]\n",
        "\n",
        "    def create_cli():\n",
        "        english = []\n",
        "        cli = []\n",
        "        for pleasantry in pleasantries:\n",
        "            for action in [\"create\", \"make\", \"build\"]:\n",
        "                for pronoun in [\"\", \"a\", \"the\", \"me a\"]:\n",
        "                    for name in [\"dir\", \"folder\", \"directory\"]:\n",
        "                        for extra in [\"\", \"called\", \"calling it\", \"named\", \"naming it\", \"label it\", \"tag it\"]:\n",
        "                            for dirname in dirnames:\n",
        "                                eng = f\"{pleasantry} {action} {pronoun} {name} {extra} {dirname}\"\n",
        "                                cmd = f\"mkdir {dirname}\"\n",
        "                                english.append(eng.strip())\n",
        "                                cli.append(cmd.strip())\n",
        "        return english, cli\n",
        "\n",
        "    def delete_cli():\n",
        "        english = []\n",
        "        cli = []\n",
        "        for pleasantry in pleasantries:\n",
        "            for action in [\"delete\", \"del\", \"purge\", \"nuke\", \"trash\", \"remove\"]:\n",
        "                for pronoun in [\"\", \"a\", \"the\"]:\n",
        "                    for obj in [\"file\", \"dir\", \"folder\", \"directory\"]:\n",
        "                        for extra in [\"\", \"named\", \"by the name\", \"with name\"]:\n",
        "                            if obj == \"file\":\n",
        "                                args = \"\"\n",
        "                                objnames = filenames\n",
        "                            else:\n",
        "                                args = \"-r\"\n",
        "                                objnames = dirnames\n",
        "                            for objname in objnames:\n",
        "                                eng = f\"{pleasantry} {action} {pronoun} {obj} {extra} {objname}\"\n",
        "                                cmd = f\"rm {args} {objname}\"\n",
        "                                english.append(eng.strip())\n",
        "                                cli.append(cmd.strip())\n",
        "        return english, cli\n",
        "\n",
        "    def rename_cli():\n",
        "        english = []\n",
        "        cli = []\n",
        "        for pleasantry in pleasantries:\n",
        "            for action in [\"rename\", \"move\", \"change\", \"replace\"]:\n",
        "                for name in [\"\", \"file\", \"the file\", \"dir\", \"the dir\" \"folder\", \"the folder\"]:\n",
        "                    for extra in [\"to\", \"as\", \"and call it\", \"to new name\", \"with\", \"and name it\"]:\n",
        "                        for src in oldnames:\n",
        "                            for dst in newnames:\n",
        "                                eng = f\"{pleasantry} {action} {name} {src} {extra} {dst}\"\n",
        "                                cmd = f\"mv {src} {dst}\"\n",
        "                                english.append(eng.strip())\n",
        "                                cli.append(cmd.strip())\n",
        "        return english, cli\n",
        "\n",
        "    def copy_cli():\n",
        "        english = []\n",
        "        cli = []\n",
        "        for pleasantry in pleasantries:\n",
        "            for action in [\"copy\", \"clone\", \"duplicate\", \"cp\"]:\n",
        "                for name in [\"\", \"file\", \"the file\", \"dir\", \"the dir\" \"folder\", \"the folder\", \"directory\", \"the directory\"]:\n",
        "                    for extra in [\"to\", \"as\"]:\n",
        "                        for src in srcs:\n",
        "                            for dst in dests:\n",
        "                                eng = f\"{pleasantry} {action} {name} {src} {extra} {dst}\"\n",
        "                                cmd = f\"cp {src} {dst}\"\n",
        "                                english.append(eng.strip())\n",
        "                                cli.append(cmd.strip())\n",
        "        return english, cli\n",
        "\n",
        "\n",
        "    def list_cli():\n",
        "        english = []\n",
        "        cli = []\n",
        "        for pleasantry in pleasantries:\n",
        "            for action in [\"list\", \"show\", \"display\", \"ls\", \"present\"]:\n",
        "                for name in [\"\", \"dir\", \"directory\", \"files under\", \"everything under\", \"files inside\", \"contents of\"]:\n",
        "                    for extra in [\"\", \"named\", \"by the name\", \"with name\"]:\n",
        "                            for dirname in dirnames:\n",
        "                                eng = f\"{pleasantry} {action} {name} {extra} {dirname}\"\n",
        "                                cmd = f\"ls -l {dirname}\"\n",
        "                                english.append(eng.strip())\n",
        "                                cli.append(cmd.strip())\n",
        "        return english, cli\n",
        "\n",
        "    def search_cli():\n",
        "        english = []\n",
        "        cli = []\n",
        "        for pleasantry in pleasantries:\n",
        "            for action in [\"find\", \"search\", \"look for\", \"search for\", \"grep\", \"check for\"]:\n",
        "                for extra1 in [\"\", \"\", \"word\", \"the word\", \"pattern\", \"existence of\"]:\n",
        "                    for needle in words:\n",
        "                        for extra2 in [\"in\", \"inside\", \"in file\"]:\n",
        "                            for haystack in filenames:\n",
        "                                eng = f\"{pleasantry} {action} {extra1} {needle} {extra2} {haystack}\"\n",
        "                                cmd = f\"grep -i {needle} {haystack}\"\n",
        "                                english.append(eng.strip())\n",
        "                                cli.append(cmd.strip())\n",
        "        return english, cli\n",
        "\n",
        "\n",
        "    english = []\n",
        "    cli = []\n",
        "\n",
        "    e, c = create_cli()\n",
        "    english.extend(e)\n",
        "    cli.extend(c)\n",
        "\n",
        "    e, c = delete_cli()\n",
        "    english.extend(e)\n",
        "    cli.extend(c)\n",
        "\n",
        "    e, c = rename_cli()\n",
        "    english.extend(e)\n",
        "    cli.extend(c)\n",
        "\n",
        "    e, c = copy_cli()\n",
        "    english.extend(e)\n",
        "    cli.extend(c)\n",
        "\n",
        "    e, c = list_cli()\n",
        "    english.extend(e)\n",
        "    cli.extend(c)\n",
        "\n",
        "    e, c = search_cli()\n",
        "    english.extend(e)\n",
        "    cli.extend(c)\n",
        "\n",
        "    indices = list(range(len(english)))\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    english = [english[i] for i in indices]\n",
        "    machine = [cli[i] for i in indices]\n",
        "\n",
        "    placeholders = set(words) | set(filenames) | set(dirnames) | set(oldnames) | set(newnames) | set(srcs) | set(dests)\n",
        "\n",
        "    return english, machine, placeholders"
      ],
      "metadata": {
        "id": "UiaNvrl46wDt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Seq2Seq to build \"Jarvis\" our Conversational Language Interface**"
      ],
      "metadata": {
        "id": "OvNrkwGTpqOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class JarvisTranslator:\n",
        "    def __init__(self, max_english_len=20, max_cli_len=10, embedding_dim=300, latent_dim=1024):\n",
        "        self.max_english_len = max_english_len\n",
        "        self.max_cli_len = max_cli_len\n",
        "        self.latent_dim = latent_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.cli_vocab_size = 0\n",
        "        self.placeholders = None\n",
        "\n",
        "        self.english_tokenizer = None\n",
        "        self.cli_tokenizer = None\n",
        "\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "        self.decoder_logits = None\n",
        "        self.training_model = None\n",
        "        self.inference_model = None\n",
        "\n",
        "    def prepare_training_data(self, english_sentences, cli_commands, placeholders):\n",
        "        self.placeholders = placeholders\n",
        "\n",
        "        # Tokenize the English/english sentences\n",
        "        self.english_tokenizer = Tokenizer(filters=\"\", lower=True)  # english Language (lower case everything)\n",
        "        self.english_tokenizer.fit_on_texts(english_sentences)\n",
        "        self.english_vocab_size = len(self.english_tokenizer.word_index) + 1\n",
        "\n",
        "        # Tokenize the CLI command sentences\n",
        "        self.cli_tokenizer   = Tokenizer(filters=\"\", lower=False)   # CLI Language (preserve case)\n",
        "        self.cli_tokenizer.fit_on_texts([\"[START]\", \"[END]\"] + cli_commands)\n",
        "        self.cli_vocab_size = len(self.cli_tokenizer.word_index) + 1\n",
        "\n",
        "        # Convert texts → integer sequences\n",
        "        en_seqs = self.english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "        cmd_in  = [\"[START]\" + \" \" + seq for seq in cli_commands]\n",
        "        cmd_out = [seq + \" \" + \"[END]\"   for seq in cli_commands]\n",
        "        cmd_seqs_in  = self.cli_tokenizer.texts_to_sequences(cmd_in)\n",
        "        cmd_seqs_out = self.cli_tokenizer.texts_to_sequences(cmd_out)\n",
        "\n",
        "        # Pad to fixed lengths\n",
        "        encoder_inputs = pad_sequences(en_seqs, maxlen=self.max_english_len, padding=\"post\")\n",
        "        decoder_inputs = pad_sequences(cmd_seqs_in,  maxlen=self.max_cli_len, padding=\"post\")\n",
        "        decoder_targets= pad_sequences(cmd_seqs_out, maxlen=self.max_cli_len, padding=\"post\")\n",
        "\n",
        "        # Expand dims for sparse_categorical_crossentropy\n",
        "        decoder_targets = np.expand_dims(decoder_targets, -1)\n",
        "\n",
        "        return encoder_inputs, decoder_inputs, decoder_targets\n",
        "\n",
        "    def build_training_model(self):\n",
        "        assert self.training_model is None, \"Training model is already built!\"\n",
        "\n",
        "        # ENCODER\n",
        "        encoder_inputs = Input(shape=(self.max_english_len,))\n",
        "        encoder_embedding = Embedding(self.english_vocab_size, self.embedding_dim, name=\"encoder_embedding\")(encoder_inputs)\n",
        "        encoder_outputs, encoder_h, encoder_c = LSTM(self.latent_dim, return_sequences=True, return_state=True, name=\"encoder_lstm\")(encoder_embedding)\n",
        "\n",
        "        self.encoder = Model(inputs=encoder_inputs, outputs=[encoder_outputs, encoder_h, encoder_c], name=\"encoder\")\n",
        "\n",
        "        # DECODER\n",
        "        decoder_inputs = Input(shape=(self.max_cli_len,), name=\"decoder\")\n",
        "        decoder_embedding = Embedding(self.cli_vocab_size, self.embedding_dim, name=\"decoder_embedding\")(decoder_inputs)\n",
        "        decoder_outputs, _, _ = LSTM(self.latent_dim, return_sequences=True, return_state=True, name=\"decoder_lstm\")(decoder_embedding, initial_state=[encoder_h, encoder_c])\n",
        "\n",
        "        # ATTENTION LAYER (Luong Attention)\n",
        "        attention_output = Attention(use_scale=True, name=\"luong_attention\")([decoder_outputs, encoder_outputs])\n",
        "\n",
        "        # Concatenate decoder outputs with attention context (Luong-style)\n",
        "        decoder_concat = Concatenate(axis=-1, name=\"context_concat\")([decoder_outputs, attention_output])\n",
        "\n",
        "        # Final output layer (project to vocab size to pick a token from across all tokens in our vocab)\n",
        "        decoder_logits = Dense(self.cli_vocab_size, activation=\"softmax\", name=\"output_logits\")(decoder_concat)\n",
        "\n",
        "        # Create training model\n",
        "        self.training_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_logits)\n",
        "\n",
        "        self.training_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "        return self.training_model\n",
        "\n",
        "    def build_inference_model(self):\n",
        "        assert self.inference_model is None, \"Inference model is already built!\"\n",
        "\n",
        "        # REUSE ENCODER\n",
        "        encoder_inputs = Input(shape=(self.max_english_len,))\n",
        "        encoder_embedding = self.training_model.get_layer(\"encoder_embedding\")(encoder_inputs)  # REUSE Encoder's Embedding Layer\n",
        "        encoder_outputs, encoder_h, encoder_c = self.training_model.get_layer(\"encoder_lstm\")(encoder_embedding) # REUSE Encoder's LSTM Layer\n",
        "\n",
        "        # REUSE *PARTS OF* DECODER\n",
        "        decoder_input = Input(shape=(1,))             # Inferencing Decoder takes one-token at a time to predict next token\n",
        "        decoder_h = Input(shape=(self.latent_dim,))   # Socket to plugin Encoder's LAST encoder_h (hidden-state) to Decoder\n",
        "        decoder_c = Input(shape=(self.latent_dim,))   # Socket to plugin Encoder's LAST encoder_c (cell-state) to Decoder\n",
        "\n",
        "        decoder_embedding = self.training_model.get_layer(\"decoder_embedding\")(decoder_input) # REUSE Decoder's Embedding Layer\n",
        "        decoder_lstm = self.training_model.get_layer(\"decoder_lstm\") # REUSE Decoder's LSTM Layer\n",
        "        decoder_output, decoder_h_out, decoder_c_out = decoder_lstm(decoder_embedding, initial_state=[decoder_h, decoder_c])\n",
        "\n",
        "\n",
        "        # REUSE ATTENTION\n",
        "        encoder_outputs_input = Input(shape=(self.max_english_len, self.latent_dim)) # Socket to plugin in all hidden-sates of Encoder to Attention Module\n",
        "        luong_attention = self.training_model.get_layer(\"luong_attention\")   # RUSE\n",
        "        attention_output = luong_attention([decoder_output, encoder_outputs_input])\n",
        "\n",
        "        context_concat = Concatenate(axis=-1)([decoder_output, attention_output])\n",
        "\n",
        "        # Final prediction (reuse trained weights)\n",
        "        output_logits = self.training_model.get_layer(\"output_logits\")(context_concat)\n",
        "\n",
        "        # Create inference model\n",
        "        self.inference_model = Model(\n",
        "            inputs=[decoder_input, encoder_outputs_input, decoder_h, decoder_c],\n",
        "            outputs=[output_logits, decoder_h_out, decoder_c_out],\n",
        "        )\n",
        "\n",
        "        return self.inference_model\n",
        "\n",
        "    def build_model(self):\n",
        "        if self.training_model is None:\n",
        "            self.build_training_model()\n",
        "\n",
        "    def show_model(self):\n",
        "        if self.training_model is not None:\n",
        "            print(\"TRAINING MODEL:\")\n",
        "            self.training_model.summary()\n",
        "\n",
        "        if self.inference_model is not None:\n",
        "            print(\"INFERENCE MODEL:\")\n",
        "            self.inference_model.summary()\n",
        "\n",
        "    def train(self, english_seqs, cli_seqs_in, cli_seqs_out, batch_size=64, epochs=20, verbose=2):\n",
        "        if self.training_model is None:\n",
        "            self.build_training_model()\n",
        "\n",
        "        trainer = self.training_model\n",
        "\n",
        "        num_samples   = len(english_seqs)\n",
        "        split         = int(0.8 * num_samples)  # Split 80% for Training and 20% for Validation\n",
        "        train_enc_in  = english_seqs[:split]\n",
        "        val_enc_in    = english_seqs[split:]\n",
        "        train_dec_in  = cli_seqs_in[:split]\n",
        "        val_dec_in    = cli_seqs_in[split:]\n",
        "        train_dec_out = cli_seqs_out[:split]\n",
        "        val_dec_out   = cli_seqs_out[split:]\n",
        "\n",
        "        history = trainer.fit([train_enc_in, train_dec_in], train_dec_out, batch_size=batch_size, epochs=epochs,\n",
        "                               validation_data=([val_enc_in, val_dec_in], val_dec_out), verbose=verbose)\n",
        "        return history\n",
        "\n",
        "    def english_to_cli(self, english_sentence):\n",
        "\n",
        "        if self.inference_model is None:\n",
        "            self.build_inference_model()\n",
        "\n",
        "        # Tokenize enlgish sentence as a sequence and pad it\n",
        "        eng_seq = self.english_tokenizer.texts_to_sequences([english_sentence.lower()])\n",
        "        eng_seq = pad_sequences(eng_seq, maxlen=self.max_english_len, padding=\"post\")\n",
        "\n",
        "        # Run it through encoder to get the encoder's output\n",
        "        encoder_outputs, encoder_hstates, encoder_cstates = self.encoder.predict(eng_seq, verbose=0)\n",
        "\n",
        "        model = self.inference_model\n",
        "\n",
        "        # Initialize the Decoder by passing it the [START] token so that it can start generating\n",
        "        tokenid = self.cli_tokenizer.word_index[\"[START]\"]\n",
        "        cli_seq = np.array([[tokenid]])\n",
        "        states  = [encoder_hstates, encoder_cstates]\n",
        "\n",
        "        decoded_tokens = []\n",
        "        for _ in range(self.max_cli_len):\n",
        "            # predict next token\n",
        "            logits, decoder_hstates, decoder_cstates = model.predict([cli_seq, encoder_outputs] + states, verbose = 0)\n",
        "\n",
        "            tokenid = np.argmax(logits[0, -1, :])\n",
        "            token = self.cli_tokenizer.index_word.get(tokenid, \"\")\n",
        "\n",
        "            if token == \"[END]\":\n",
        "                break\n",
        "            if token and token != \"[START]\":\n",
        "                decoded_tokens.append(token)\n",
        "\n",
        "            cli_seq = np.array([[tokenid]])\n",
        "            states  = [decoder_hstates, decoder_cstates]\n",
        "\n",
        "\n",
        "        # extract parameter (argument) values from the english sentence\n",
        "        raw_tokens = english_sentence.strip().split()\n",
        "        input_oov = [tok for tok in raw_tokens if tok.lower() not in self.english_tokenizer.word_index]\n",
        "\n",
        "        final_tokens = []\n",
        "        oov_queue = input_oov.copy()\n",
        "        for tok in decoded_tokens:\n",
        "            if tok in self.placeholders and oov_queue:\n",
        "                final_tokens.append(oov_queue.pop(0))\n",
        "            else:\n",
        "                final_tokens.append(tok)\n",
        "\n",
        "        return \" \".join(final_tokens)\n"
      ],
      "metadata": {
        "id": "2dPNwfMf6mE9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "english_sentences, cli_commands, placeholders = generate_english_to_cli_sentences()\n",
        "\n",
        "print(\"Jarvis will be trained on\", len(english_sentences), \"English sentences\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrVSklvL608L",
        "outputId": "b48a0210-32aa-4a79-f6bd-7884d40f8ea9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jarvis will be trained on 56500 English sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    print(\"{:<6} | {:55s} | {}\".format(i+1, english_sentences[i], cli_commands[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TsmG_jyochgK",
        "outputId": "6d32a1bc-a0b9-40b4-c0cd-6eb2ef28a525"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1      | kindly move  HQLY and name it LFY                       | mv HQLY LFY\n",
            "2      | hey rename the folder DFADM and name it TWV             | mv DFADM TWV\n",
            "3      | kindly check for pattern XTY in BSPD                    | grep -i XTY BSPD\n",
            "4      | could you purge the file named MYOGG                    | rm  MYOGG\n",
            "5      | duplicate  RAOA to WRV                                  | cp RAOA WRV\n",
            "6      | create me a directory called IZJQRK                     | mkdir IZJQRK\n",
            "7      | kindly change  DFADM to new name WRWN                   | mv DFADM WRWN\n",
            "8      | hey trash  directory by the name JEOMEV                 | rm -r JEOMEV\n",
            "9      | change the folder EIN as TWV                            | mv EIN TWV\n",
            "10     | please move  KBJI and call it WRWN                      | mv KBJI WRWN\n",
            "11     | kindly nuke  dir by the name JEOMEV                     | rm -r JEOMEV\n",
            "12     | hey change the dirfolder KBJI and call it LFY           | mv KBJI LFY\n",
            "13     | hey search for pattern IZNO in MYOGG                    | grep -i IZNO MYOGG\n",
            "14     | kindly change dir HQLY to LFY                           | mv HQLY LFY\n",
            "15     | kindly remove the dir named XWMH                        | rm -r XWMH\n",
            "16     | could you check for  MECJ inside LRVY                   | grep -i MECJ LRVY\n",
            "17     | please move file KBJI to NHCZLH                         | mv KBJI NHCZLH\n",
            "18     | hey create a dir calling it LAI                         | mkdir LAI\n",
            "19     | could you present files inside  JEOMEV                  | ls -l JEOMEV\n",
            "20     | hey change file DFADM and name it LFY                   | mv DFADM LFY\n",
            "21     | move  OUUQIB with NHCZLH                                | mv OUUQIB NHCZLH\n",
            "22     | please build a directory label it JEOMEV                | mkdir JEOMEV\n",
            "23     | kindly create a directory named XWMH                    | mkdir XWMH\n",
            "24     | hey move the file OUUQIB with RLZEOS                    | mv OUUQIB RLZEOS\n",
            "25     | kindly create me a dir tag it JEOMEV                    | mkdir JEOMEV\n",
            "26     | clone the dirfolder RAOA to QRKTTI                      | cp RAOA QRKTTI\n",
            "27     | please make a folder named IZJQRK                       | mkdir IZJQRK\n",
            "28     | kindly rename the dirfolder EIN and name it WRWN        | mv EIN WRWN\n",
            "29     | trash the dir by the name JEOMEV                        | rm -r JEOMEV\n",
            "30     | kindly search for existence of ARVD in MYOGG            | grep -i ARVD MYOGG\n",
            "31     | kindly change dir OUUQIB as RLZEOS                      | mv OUUQIB RLZEOS\n",
            "32     | move  KBJI to new name WRWN                             | mv KBJI WRWN\n",
            "33     | hey purge a dir named JEOMEV                            | rm -r JEOMEV\n",
            "34     | search pattern XTY in TWDKVF                            | grep -i XTY TWDKVF\n",
            "35     | please change the folder HQLY and name it RLZEOS        | mv HQLY RLZEOS\n",
            "36     | could you copy the directory TMUED to DANYAT            | cp TMUED DANYAT\n",
            "37     | hey copy the dirfolder TMUED as FJMLC                   | cp TMUED FJMLC\n",
            "38     | copy the dirfolder ZEXD to WRV                          | cp ZEXD WRV\n",
            "39     | kindly change dir EIN to new name RLZEOS                | mv EIN RLZEOS\n",
            "40     | please rename the file DFADM to WRWN                    | mv DFADM WRWN\n",
            "41     | please rename file DFADM and call it RLZEOS             | mv DFADM RLZEOS\n",
            "42     | build me a folder tag it XWMH                           | mkdir XWMH\n",
            "43     | build the directory tag it JEOMEV                       | mkdir JEOMEV\n",
            "44     | grep word MZAVEF in LRVY                                | grep -i MZAVEF LRVY\n",
            "45     | hey replace the folder KBJI and call it RLZEOS          | mv KBJI RLZEOS\n",
            "46     | hey replace  HQLY to RLZEOS                             | mv HQLY RLZEOS\n",
            "47     | kindly rename file EIN and name it NHCZLH               | mv EIN NHCZLH\n",
            "48     | could you move dir DFADM with WRWN                      | mv DFADM WRWN\n",
            "49     | please duplicate directory DZLW as DANYAT               | cp DZLW DANYAT\n",
            "50     | duplicate the file ZEXD as QRKTTI                       | cp ZEXD QRKTTI\n",
            "51     | kindly search for pattern IZNO in SFFPO                 | grep -i IZNO SFFPO\n",
            "52     | could you show  named IZJQRK                            | ls -l IZJQRK\n",
            "53     | hey replace file OUUQIB to LFY                          | mv OUUQIB LFY\n",
            "54     | please create me a directory label it LAI               | mkdir LAI\n",
            "55     | kindly del  folder with name IZJQRK                     | rm -r IZJQRK\n",
            "56     | kindly replace file KBJI and name it RLZEOS             | mv KBJI RLZEOS\n",
            "57     | could you present directory named XWMH                  | ls -l XWMH\n",
            "58     | copy  DZLW as FJMLC                                     | cp DZLW FJMLC\n",
            "59     | kindly search for the word IZNO in MYOGG                | grep -i IZNO MYOGG\n",
            "60     | hey create the dir tag it XWMH                          | mkdir XWMH\n",
            "61     | kindly delete  file by the name SFFPO                   | rm  SFFPO\n",
            "62     | please copy the directory IXC as UTJCOW                 | cp IXC UTJCOW\n",
            "63     | hey clone directory RAOA to WRV                         | cp RAOA WRV\n",
            "64     | please grep existence of XTY in file SFFPO              | grep -i XTY SFFPO\n",
            "65     | hey create a directory tag it PNC                       | mkdir PNC\n",
            "66     | hey copy directory RAOA as FJMLC                        | cp RAOA FJMLC\n",
            "67     | could you replace dir KBJI to RLZEOS                    | mv KBJI RLZEOS\n",
            "68     | hey search existence of XTY in file LRVY                | grep -i XTY LRVY\n",
            "69     | move dir DFADM to NHCZLH                                | mv DFADM NHCZLH\n",
            "70     | could you check for  IZNO in MYOGG                      | grep -i IZNO MYOGG\n",
            "71     | hey search the word IZNO in file TWDKVF                 | grep -i IZNO TWDKVF\n",
            "72     | move the file DFADM and name it TWV                     | mv DFADM TWV\n",
            "73     | kindly clone the dirfolder IXC to WRV                   | cp IXC WRV\n",
            "74     | hey duplicate  DZLW to QRKTTI                           | cp DZLW QRKTTI\n",
            "75     | could you copy the directory IXC as FJMLC               | cp IXC FJMLC\n",
            "76     | could you build  dir naming it IZJQRK                   | mkdir IZJQRK\n",
            "77     | kindly purge  file  MYOGG                               | rm  MYOGG\n",
            "78     | kindly change file EIN and name it NHCZLH               | mv EIN NHCZLH\n",
            "79     | kindly move  EIN to RLZEOS                              | mv EIN RLZEOS\n",
            "80     | please rename the file DFADM and name it WRWN           | mv DFADM WRWN\n",
            "81     | purge a directory named JEOMEV                          | rm -r JEOMEV\n",
            "82     | kindly clone the directory DZLW as UTJCOW               | cp DZLW UTJCOW\n",
            "83     | hey remove  directory  IZJQRK                           | rm -r IZJQRK\n",
            "84     | could you make  dir named PNC                           | mkdir PNC\n",
            "85     | could you find existence of IZNO in file MYOGG          | grep -i IZNO MYOGG\n",
            "86     | kindly replace the folder KBJI with WRWN                | mv KBJI WRWN\n",
            "87     | hey show contents of with name XWMH                     | ls -l XWMH\n",
            "88     | kindly delete  file  TWDKVF                             | rm  TWDKVF\n",
            "89     | please make the dir named XWMH                          | mkdir XWMH\n",
            "90     | could you change the dirfolder KBJI to TWV              | mv KBJI TWV\n",
            "91     | hey clone the file TMUED as QRKTTI                      | cp TMUED QRKTTI\n",
            "92     | build  directory calling it PNC                         | mkdir PNC\n",
            "93     | could you check for pattern IZNO inside SFFPO           | grep -i IZNO SFFPO\n",
            "94     | please look for  MECJ in file LRVY                      | grep -i MECJ LRVY\n",
            "95     | could you rename the file OUUQIB to NHCZLH              | mv OUUQIB NHCZLH\n",
            "96     | hey present directory  JEOMEV                           | ls -l JEOMEV\n",
            "97     | please show directory with name JEOMEV                  | ls -l JEOMEV\n",
            "98     | could you display directory with name JEOMEV            | ls -l JEOMEV\n",
            "99     | hey trash  dir named XWMH                               | rm -r XWMH\n",
            "100    | kindly make  directory tag it LAI                       | mkdir LAI\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jarvis = JarvisTranslator()\n",
        "\n",
        "english_seqs, cli_seqs_in, cli_seqs_out = jarvis.prepare_training_data(english_sentences, cli_commands, placeholders)\n",
        "\n",
        "jarvis.build_model()\n",
        "jarvis.show_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "TtD6rYDpFrud",
        "outputId": "1c11112d-a887-4cff-984f-b14c2071f398"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAINING MODEL:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │     \u001b[38;5;34m30,600\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │     \u001b[38;5;34m14,100\u001b[0m │ decoder[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m,       │  \u001b[38;5;34m5,427,200\u001b[0m │ encoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │                   │\n",
              "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │                   │\n",
              "│                     │ \u001b[38;5;34m1024\u001b[0m)]            │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m) │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m,       │  \u001b[38;5;34m5,427,200\u001b[0m │ decoder_embeddin… │\n",
              "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,     │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m1024\u001b[0m)]            │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ luong_attention     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m1024\u001b[0m)  │          \u001b[38;5;34m1\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mAttention\u001b[0m)         │                   │            │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context_concat      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m2048\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ luong_attention[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_logits       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m47\u001b[0m)    │     \u001b[38;5;34m96,303\u001b[0m │ context_concat[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">30,600</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_embedding   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │     <span style=\"color: #00af00; text-decoration-color: #00af00\">14,100</span> │ decoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,427,200</span> │ encoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │                   │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]            │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>) │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>,       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">5,427,200</span> │ decoder_embeddin… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)]            │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ luong_attention     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Attention</span>)         │                   │            │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context_concat      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ luong_attention[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ output_logits       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">96,303</span> │ context_concat[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,995,404\u001b[0m (41.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,995,404</span> (41.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m10,995,404\u001b[0m (41.94 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,995,404</span> (41.94 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Jarvis**"
      ],
      "metadata": {
        "id": "Hw95lVJCqcMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "jarvis.train(english_seqs, cli_seqs_in, cli_seqs_out, batch_size=64, epochs=10, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6A-AGTL6248",
        "outputId": "24b398d7-82f6-471d-8922-b6cd1e26cbc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "707/707 - 11s - 16ms/step - accuracy: 0.9323 - loss: 0.1904 - val_accuracy: 0.9672 - val_loss: 0.1246\n",
            "Epoch 2/10\n",
            "707/707 - 7s - 9ms/step - accuracy: 0.9947 - loss: 0.0106 - val_accuracy: 1.0000 - val_loss: 1.2178e-04\n",
            "Epoch 3/10\n",
            "707/707 - 7s - 9ms/step - accuracy: 1.0000 - loss: 6.8610e-05 - val_accuracy: 1.0000 - val_loss: 3.8937e-05\n",
            "Epoch 4/10\n",
            "707/707 - 7s - 9ms/step - accuracy: 1.0000 - loss: 2.7087e-05 - val_accuracy: 1.0000 - val_loss: 1.8616e-05\n",
            "Epoch 5/10\n",
            "707/707 - 6s - 9ms/step - accuracy: 1.0000 - loss: 1.4032e-05 - val_accuracy: 1.0000 - val_loss: 1.0389e-05\n",
            "Epoch 6/10\n",
            "707/707 - 6s - 9ms/step - accuracy: 1.0000 - loss: 8.1078e-06 - val_accuracy: 1.0000 - val_loss: 6.1985e-06\n",
            "Epoch 7/10\n",
            "707/707 - 6s - 9ms/step - accuracy: 1.0000 - loss: 4.9467e-06 - val_accuracy: 1.0000 - val_loss: 3.9020e-06\n",
            "Epoch 8/10\n",
            "707/707 - 6s - 9ms/step - accuracy: 1.0000 - loss: 3.1922e-06 - val_accuracy: 1.0000 - val_loss: 2.5673e-06\n",
            "Epoch 9/10\n",
            "707/707 - 6s - 9ms/step - accuracy: 1.0000 - loss: 2.1188e-06 - val_accuracy: 1.0000 - val_loss: 1.7223e-06\n",
            "Epoch 10/10\n",
            "707/707 - 6s - 9ms/step - accuracy: 1.0000 - loss: 1.4325e-06 - val_accuracy: 1.0000 - val_loss: 1.1733e-06\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7cbd66702750>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Use Jarvis to turn English sentences into Linux commands**"
      ],
      "metadata": {
        "id": "K9spwiOTqfJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentences = [\n",
        "    \"hey create me a folder name ai\",\n",
        "    \"please rename the directory ai to ai-presentations\",\n",
        "    \"copy file seq2seq.pptx to folder ai-presentations\",\n",
        "    \"kindly rename it seq2seq.pptx to encoder-decoder.pptx\",\n",
        "    \"display files under ai-presentations\",\n",
        "    \"search for the word attention in the file myreport.pdf\",\n",
        "    \"could you rename myreport.pdf to hisreport.pdf\",\n",
        "    \"replace hisreport.pdf to myreport.pdf\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    cli = jarvis.english_to_cli(sentence)\n",
        "    print(\"{:60s} \\u2192   {}\".format(sentence, cli))\n",
        "    print(\"-\" * 100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBcvozdJ64p2",
        "outputId": "bba26077-b6c2-413b-96cc-177abaa1ed22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey create me a folder name ai                               →   mkdir ai\n",
            "----------------------------------------------------------------------------------------------------\n",
            "please rename the directory ai to ai-presentations           →   mv ai ai-presentations\n",
            "----------------------------------------------------------------------------------------------------\n",
            "copy file seq2seq.pptx to folder ai-presentations            →   mkdir seq2seq.pptx\n",
            "----------------------------------------------------------------------------------------------------\n",
            "kindly rename it seq2seq.pptx to encoder-decoder.pptx        →   mv seq2seq.pptx encoder-decoder.pptx\n",
            "----------------------------------------------------------------------------------------------------\n",
            "display files under ai-presentations                         →   ls -l ai-presentations\n",
            "----------------------------------------------------------------------------------------------------\n",
            "search for the word attention in the file myreport.pdf       →   grep -i attention myreport.pdf\n",
            "----------------------------------------------------------------------------------------------------\n",
            "could you rename myreport.pdf to hisreport.pdf               →   mv myreport.pdf hisreport.pdf\n",
            "----------------------------------------------------------------------------------------------------\n",
            "replace hisreport.pdf to myreport.pdf                        →   mv hisreport.pdf myreport.pdf\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}