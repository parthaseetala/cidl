{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO16XYSMg7xOfGYu7xW+Hq8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Fun with Word Embeddings**\n",
        "#### **Author: Partha Seetala**\n",
        "\n",
        "**Video Tutorial: https://www.youtube.com/watch?v=8jqqE8XG5T0**\n"
      ],
      "metadata": {
        "id": "2jEHDZbI21nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download pre-built Word Embeddings**\n",
        "\n"
      ],
      "metadata": {
        "id": "zQBpX5eogtdf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EYnC6KSX27VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (gensim and numpy)\n",
        "!pip install gensim -U\n",
        "!pip install numpy==1.25\n",
        "!pip install keras_preprocessing"
      ],
      "metadata": {
        "id": "JypJihkFh3cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "001e2242-d8cc-4507-da53-e42dd3043854"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.25.0)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: numpy==1.25 in /usr/local/lib/python3.11/dist-packages (1.25.0)\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from keras_preprocessing) (1.25.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from keras_preprocessing) (1.17.0)\n",
            "Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2vec = None\n",
        "glove = None\n",
        "embeddings = None"
      ],
      "metadata": {
        "id": "6wObpzhGg2_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "\n",
        "if w2vec is None:\n",
        "    # Download the 300 dimension Word2Vec Embeddings built from Google News\n",
        "    print(\"Downloading Word2Vec embeddings that were built from Google News\")\n",
        "    w2vec = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "if glove is None:\n",
        "    # Download the 300 dimension GloVe Embeddings\n",
        "    print(\"Downloading GloVe embeddings that were built from Wikipedia\")\n",
        "    glove = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufdkOMd05Rjl",
        "outputId": "a7b6e01a-443a-4d5c-af6a-3365a6eb69a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading GloVe embeddings that were built from Wikipedia\n",
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# select which embedding we'll use\n",
        "embeddings = glove"
      ],
      "metadata": {
        "id": "VAUYADuNh_Jf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUN 1**: Finding how similar are two words"
      ],
      "metadata": {
        "id": "JFaMd8mi8G5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_pairs = [\n",
        "    ('apple', 'orange'),\n",
        "    ('apple', 'microsoft'),\n",
        "    ('google', 'microsoft'),\n",
        "    ('man', 'father'),\n",
        "    ('woman', 'father'),\n",
        "    ('woman', 'mother'),\n",
        "    ('dog', 'wolf'),\n",
        "    ('cat', 'kitten'),\n",
        "    ('cat', 'lion'),\n",
        "    ('cat', 'dog'),\n",
        "    ('cat', 'mouse'),\n",
        "    ('cat', 'pizza'),\n",
        "    ('apple', 'mango'),\n",
        "    ('mango', 'orange'),\n",
        "    ('lemon', 'orange'),\n",
        "    ('lemon', 'lime'),\n",
        "    ('einstein', 'grape'),\n",
        "]\n",
        "\n",
        "for pair in word_pairs:\n",
        "    similarity = embeddings.similarity(pair[0], pair[1])\n",
        "    print(\"Similarity between {:>10s} and {:10s} = {:3.2f}%\".format(pair[0], pair[1], similarity*100))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTHwBY457yS1",
        "outputId": "e07514f9-ac76-4b44-c5b3-8e97d1d139cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between      apple and orange     = 32.06%\n",
            "Similarity between      apple and microsoft  = 56.64%\n",
            "Similarity between     google and microsoft  = 63.93%\n",
            "Similarity between        man and father     = 54.52%\n",
            "Similarity between      woman and father     = 45.41%\n",
            "Similarity between      woman and mother     = 68.99%\n",
            "Similarity between        dog and wolf       = 44.64%\n",
            "Similarity between        cat and kitten     = 43.05%\n",
            "Similarity between        cat and lion       = 38.01%\n",
            "Similarity between        cat and dog        = 68.17%\n",
            "Similarity between        cat and mouse      = 45.38%\n",
            "Similarity between        cat and pizza      = 14.63%\n",
            "Similarity between      apple and mango      = 40.26%\n",
            "Similarity between      mango and orange     = 35.46%\n",
            "Similarity between      lemon and orange     = 47.24%\n",
            "Similarity between      lemon and lime       = 71.73%\n",
            "Similarity between   einstein and grape      = 4.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUN 2**: Finding a word that doesn't belong"
      ],
      "metadata": {
        "id": "gjY-4eeFBco3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\n",
        "    ['artificial', 'intelligence', 'machine', 'banana'],\n",
        "    ['computer', 'laptop', 'book', 'server'],\n",
        "    ['cat', 'dog', 'lion', 'mouse'],\n",
        "    ['cat', 'dog', 'lion', 'kitten'],\n",
        "    ['rat', 'elephant', 'dinosaurs', 'giraffe'],\n",
        "    ['elephant', 'lion', 'giraffe', 'fish'],\n",
        "    ['elephant', 'lion', 'giraffe', 'fish', 'computer', 'laptop', 'book', 'water'],\n",
        "    ['gandhi', 'hitler', 'mussolini']\n",
        "]\n",
        "\n",
        "# Given a set of words find a word that doesn't belong\n",
        "for word in words:\n",
        "    odd_word_out = embeddings.doesnt_match(word)\n",
        "    print(\"{}  ->  {}\".format(word, odd_word_out))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMDqQJVMBcYA",
        "outputId": "61db033e-ab6c-4f70-b724-72cffe08c97f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'machine', 'banana']  ->  banana\n",
            "['computer', 'laptop', 'book', 'server']  ->  book\n",
            "['cat', 'dog', 'lion', 'mouse']  ->  mouse\n",
            "['cat', 'dog', 'lion', 'kitten']  ->  lion\n",
            "['rat', 'elephant', 'dinosaurs', 'giraffe']  ->  rat\n",
            "['elephant', 'lion', 'giraffe', 'fish']  ->  fish\n",
            "['elephant', 'lion', 'giraffe', 'fish', 'computer', 'laptop', 'book', 'water']  ->  laptop\n",
            "['gandhi', 'hitler', 'mussolini']  ->  gandhi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUN 3**: Finding other words similar to a given word"
      ],
      "metadata": {
        "id": "4DfsTtb2MhrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"human\", \"computer\", \"intelligence\", \"pizza\"]\n",
        "\n",
        "for word in words:\n",
        "    similar_words = embeddings.most_similar(word, topn=10)\n",
        "    print(\"\\n\", word)\n",
        "    for sim in similar_words:\n",
        "        print(\" - {} {:3.2f}%\".format(sim[0], sim[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Jteh5RuMnYU",
        "outputId": "32dd87d0-dba1-4df9-9956-57dd5252c355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " human\n",
            " - rights 66.78%\n",
            " - beings 62.11%\n",
            " - animal 58.78%\n",
            " - humans 56.96%\n",
            " - humanity 50.16%\n",
            " - animals 49.98%\n",
            " - environmental 47.70%\n",
            " - abuses 47.38%\n",
            " - democracy 46.17%\n",
            " - scientists 45.76%\n",
            "\n",
            " computer\n",
            " - computers 82.48%\n",
            " - software 73.34%\n",
            " - pc 62.40%\n",
            " - technology 61.99%\n",
            " - computing 61.79%\n",
            " - laptop 59.56%\n",
            " - internet 58.58%\n",
            " - ibm 58.25%\n",
            " - systems 57.45%\n",
            " - hardware 57.29%\n",
            "\n",
            " intelligence\n",
            " - cia 65.24%\n",
            " - information 55.40%\n",
            " - security 54.01%\n",
            " - counterterrorism 53.87%\n",
            " - operatives 53.31%\n",
            " - fbi 53.30%\n",
            " - military 53.12%\n",
            " - secret 52.89%\n",
            " - spy 52.25%\n",
            " - agency 50.99%\n",
            "\n",
            " pizza\n",
            " - pizzas 65.33%\n",
            " - taco 61.55%\n",
            " - pepperoni 60.20%\n",
            " - sandwiches 58.44%\n",
            " - restaurant 56.44%\n",
            " - sandwich 55.62%\n",
            " - burgers 53.97%\n",
            " - pasta 53.56%\n",
            " - hamburgers 53.32%\n",
            " - burger 53.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUN 4**: Adding and subtracting Word Embeddings"
      ],
      "metadata": {
        "id": "WgUgO21NNUFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll do A - B + C\n",
        "combinations = [\n",
        "    ('king', 'man', 'woman'),    # king - man + women => queen\n",
        "    ('paris', 'france', 'germany'),\n",
        "    ('brother', 'man', 'woman'),\n",
        "    ('walking', 'walk', 'swim'),\n",
        "    ('doctor', 'medicine', 'car'),\n",
        "    ('driver', 'car', 'plane'),\n",
        "    ('husband', 'man', 'woman'),\n",
        "    ('apple', 'fruit', 'vegetable'),\n",
        "    ('germany', 'hitler', 'mussolini')\n",
        "]\n",
        "\n",
        "for a, b, c in combinations:\n",
        "    result = embeddings.most_similar(positive=[a, c], negative=[b], topn=1)\n",
        "    print(\"{:>10s} - {:10s} + {:10s} = {}\".format(a, b, c, result[0][0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZpNd7t3NbNe",
        "outputId": "48200dcd-5255-4b88-dd45-e1b33a628c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      king - man        + woman      = queen\n",
            "     paris - france     + germany    = berlin\n",
            "   brother - man        + woman      = daughter\n",
            "   walking - walk       + swim       = swimming\n",
            "    doctor - medicine   + car        = driver\n",
            "    driver - car        + plane      = flight\n",
            "   husband - man        + woman      = wife\n",
            "     apple - fruit      + vegetable  = macintosh\n",
            "   germany - hitler     + mussolini  = italy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List of complex analogies to solve (with multiple additions and subtractions)\n",
        "complex_analogies = [\n",
        "    (['france', 'germany', 'italy'], ['paris', 'berlin']),\n",
        "    (['brazil', 'yen', 'rupee'], ['japan', 'india']),\n",
        "    (['walking', 'run', 'swimming'], ['walk', 'swim']),\n",
        "    (['brother', 'queen'], ['king', 'sister']),\n",
        "    (['husband', 'woman'], ['man']),\n",
        "    (['woman', 'husband'], ['man']),\n",
        "    (['cars', 'wings', 'sky'], ['wheels', 'road'])\n",
        "]\n",
        "\n",
        "# Solve each analogy\n",
        "for positive, negative in complex_analogies:\n",
        "    result = embeddings.most_similar(positive=positive, negative=negative, topn=2)\n",
        "    print(\"({}) - ({}) => \".format(\", \".join(positive), \", \".join(negative)), end=\"\")\n",
        "    for sim in result:\n",
        "        print(\"{} {:3.2f}%\".format(sim[0], sim[1]*100), end=\", \")\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FHGGkc3SALH",
        "outputId": "30d2d01e-5265-4e74-b8a4-ef697fc5ec97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(france, germany, italy) - (paris, berlin) => spain 53.19%, slovakia 49.63%, \n",
            "\n",
            "(brazil, yen, rupee) - (japan, india) => peso 61.34%, franc 53.54%, \n",
            "\n",
            "(walking, run, swimming) - (walk, swim) => running 55.06%, ran 43.81%, \n",
            "\n",
            "(brother, queen) - (king, sister) => honours 29.17%, godoy 28.37%, \n",
            "\n",
            "(husband, woman) - (man) => wife 77.33%, mother 71.96%, \n",
            "\n",
            "(woman, husband) - (man) => wife 77.33%, mother 71.96%, \n",
            "\n",
            "(cars, wings, sky) - (wheels, road) => skies 44.92%, blue 42.24%, \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUN 5**: Finding similar sentences using just Word Embeddings"
      ],
      "metadata": {
        "id": "bSHsxtI0fDjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "\n",
        "# Function to compute sentence embedding by averaging word embeddings\n",
        "def get_sentence_embedding(embeddings, sentence):\n",
        "    # Simple preprocessing: lowercase and split into words\n",
        "    words = re.sub(r'[^\\w\\s]', '', sentence.lower()).split()\n",
        "\n",
        "    # Get embeddings for words in the sentence that exist in the model\n",
        "    word_embeddings = [embeddings[word] for word in words if word in embeddings]\n",
        "\n",
        "    # Average the word embeddings to get the sentence embeddings\n",
        "    return np.mean(word_embeddings, axis=0)\n",
        "\n",
        "# Calculate a matrix that does cosine similarities across all sentences\n",
        "def calculate_similarities_among_sentences(embeddings, sentences):\n",
        "    sentence_embeddings = np.array([get_sentence_embedding(embeddings, sentence) for sentence in sentences])\n",
        "    similarity_matrix = cosine_similarity(sentence_embeddings)\n",
        "    return similarity_matrix\n",
        "\n",
        "# Print similar sentences\n",
        "def show_similar_sentences(similarity_matrix, sentences, threshold=0.75):\n",
        "    for i in range(len(sentences)):\n",
        "        similarities = []\n",
        "        for j in range(len(sentences)):\n",
        "            if i == j:\n",
        "                continue\n",
        "            sim = similarity_matrix[i][j]\n",
        "            if sim < threshold:\n",
        "                continue\n",
        "            similarities.append((sentences[j], sim))\n",
        "\n",
        "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(\"\\n\", sentences[i])\n",
        "        for sim in similarities:\n",
        "            print(\" - {} {:3.2f}%\".format(sim[0], sim[1]*100))"
      ],
      "metadata": {
        "id": "uF3f7KQ7cFIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"The quarterback threw a perfect pass to win the game in overtime.\",  # Sports\n",
        "    \"Artificial neural networks are accelerating breakthroughs in pattern recognition.\",  # Tech\n",
        "    \"The senator proposed a bill to reform healthcare funding nationwide.\",  # Politics\n",
        "    \"Philosophers debate whether free will exists or if fate governs our actions.\",  # Philosophy\n",
        "    \"A new surgical technique reduced recovery time for knee replacements.\",  # Medicine\n",
        "    \"Quantum encryption could soon make data breaches a thing of the past.\",  # Tech\n",
        "    \"The marathon runner trained for months to qualify for the Olympics.\",  # Sports\n",
        "    \"Existentialism suggests that meaning in life is created, not discovered.\",  # Philosophy\n",
        "    \"The prime minister faced criticism for delaying the climate change vote.\",  # Politics\n",
        "    \"Gene editing tools like CRISPR are revolutionizing treatment for genetic disorders.\",  # Medicine\n",
        "    \"The basketball team’s defense strategy turned the match in their favor.\",  # Sports\n",
        "    \"Tech startups are racing to develop sustainable battery alternatives.\",  # Tech\n",
        "    \"The ethics of artificial intelligence challenge our notions of responsibility.\",  # Philosophy\n",
        "    \"Voters turned out in record numbers to support the new tax initiative.\",  # Politics\n",
        "    \"A groundbreaking study linked diet to improved mental health outcomes.\",  # Medicine\n",
        "    \"The tennis champion’s backhand was unbeatable during the final set.\",  # Sports\n",
        "    \"Blockchain technology is reshaping how we verify digital transactions.\",  # Tech\n",
        "    \"Stoicism teaches us to focus only on what we can control.\",  # Philosophy\n",
        "    \"The opposition party rallied against cuts to public education spending.\",  # Politics\n",
        "    \"Antiviral drugs are being tested to shorten the duration of flu symptoms.\"  # Medicine\n",
        "]\n",
        "\n",
        "\n",
        "similarities = calculate_similarities_among_sentences(embeddings, sentences)\n",
        "show_similar_sentences(similarities, sentences, threshold=0.75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-63K2gNVSvw",
        "outputId": "9dfc26ce-2876-4792-f495-e9839d73e164"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " The quarterback threw a perfect pass to win the game in overtime.\n",
            " - The basketball team’s defense strategy turned the match in their favor. 80.72%\n",
            " - The tennis champion’s backhand was unbeatable during the final set. 76.10%\n",
            "\n",
            " Artificial neural networks are accelerating breakthroughs in pattern recognition.\n",
            "\n",
            " The senator proposed a bill to reform healthcare funding nationwide.\n",
            " - The opposition party rallied against cuts to public education spending. 83.87%\n",
            " - Voters turned out in record numbers to support the new tax initiative. 81.71%\n",
            " - The prime minister faced criticism for delaying the climate change vote. 75.26%\n",
            "\n",
            " Philosophers debate whether free will exists or if fate governs our actions.\n",
            " - Stoicism teaches us to focus only on what we can control. 84.05%\n",
            " - Existentialism suggests that meaning in life is created, not discovered. 78.33%\n",
            " - The ethics of artificial intelligence challenge our notions of responsibility. 76.70%\n",
            " - Quantum encryption could soon make data breaches a thing of the past. 75.19%\n",
            "\n",
            " A new surgical technique reduced recovery time for knee replacements.\n",
            "\n",
            " Quantum encryption could soon make data breaches a thing of the past.\n",
            " - Stoicism teaches us to focus only on what we can control. 79.88%\n",
            " - Blockchain technology is reshaping how we verify digital transactions. 77.00%\n",
            " - Voters turned out in record numbers to support the new tax initiative. 76.92%\n",
            " - Existentialism suggests that meaning in life is created, not discovered. 76.07%\n",
            " - The ethics of artificial intelligence challenge our notions of responsibility. 75.29%\n",
            " - Philosophers debate whether free will exists or if fate governs our actions. 75.19%\n",
            "\n",
            " The marathon runner trained for months to qualify for the Olympics.\n",
            " - Voters turned out in record numbers to support the new tax initiative. 76.72%\n",
            " - The basketball team’s defense strategy turned the match in their favor. 76.48%\n",
            " - The tennis champion’s backhand was unbeatable during the final set. 75.44%\n",
            "\n",
            " Existentialism suggests that meaning in life is created, not discovered.\n",
            " - Stoicism teaches us to focus only on what we can control. 80.30%\n",
            " - Philosophers debate whether free will exists or if fate governs our actions. 78.33%\n",
            " - Quantum encryption could soon make data breaches a thing of the past. 76.07%\n",
            "\n",
            " The prime minister faced criticism for delaying the climate change vote.\n",
            " - The opposition party rallied against cuts to public education spending. 80.21%\n",
            " - Voters turned out in record numbers to support the new tax initiative. 77.58%\n",
            " - The senator proposed a bill to reform healthcare funding nationwide. 75.26%\n",
            "\n",
            " Gene editing tools like CRISPR are revolutionizing treatment for genetic disorders.\n",
            "\n",
            " The basketball team’s defense strategy turned the match in their favor.\n",
            " - The quarterback threw a perfect pass to win the game in overtime. 80.72%\n",
            " - Voters turned out in record numbers to support the new tax initiative. 80.60%\n",
            " - The tennis champion’s backhand was unbeatable during the final set. 79.00%\n",
            " - The marathon runner trained for months to qualify for the Olympics. 76.48%\n",
            " - Stoicism teaches us to focus only on what we can control. 75.25%\n",
            "\n",
            " Tech startups are racing to develop sustainable battery alternatives.\n",
            "\n",
            " The ethics of artificial intelligence challenge our notions of responsibility.\n",
            " - Stoicism teaches us to focus only on what we can control. 77.22%\n",
            " - Philosophers debate whether free will exists or if fate governs our actions. 76.70%\n",
            " - Quantum encryption could soon make data breaches a thing of the past. 75.29%\n",
            "\n",
            " Voters turned out in record numbers to support the new tax initiative.\n",
            " - The opposition party rallied against cuts to public education spending. 83.71%\n",
            " - The senator proposed a bill to reform healthcare funding nationwide. 81.71%\n",
            " - The basketball team’s defense strategy turned the match in their favor. 80.60%\n",
            " - Stoicism teaches us to focus only on what we can control. 79.99%\n",
            " - The prime minister faced criticism for delaying the climate change vote. 77.58%\n",
            " - Quantum encryption could soon make data breaches a thing of the past. 76.92%\n",
            " - The marathon runner trained for months to qualify for the Olympics. 76.72%\n",
            "\n",
            " A groundbreaking study linked diet to improved mental health outcomes.\n",
            "\n",
            " The tennis champion’s backhand was unbeatable during the final set.\n",
            " - The basketball team’s defense strategy turned the match in their favor. 79.00%\n",
            " - The quarterback threw a perfect pass to win the game in overtime. 76.10%\n",
            " - The marathon runner trained for months to qualify for the Olympics. 75.44%\n",
            "\n",
            " Blockchain technology is reshaping how we verify digital transactions.\n",
            " - Quantum encryption could soon make data breaches a thing of the past. 77.00%\n",
            "\n",
            " Stoicism teaches us to focus only on what we can control.\n",
            " - Philosophers debate whether free will exists or if fate governs our actions. 84.05%\n",
            " - Existentialism suggests that meaning in life is created, not discovered. 80.30%\n",
            " - Voters turned out in record numbers to support the new tax initiative. 79.99%\n",
            " - Quantum encryption could soon make data breaches a thing of the past. 79.88%\n",
            " - The ethics of artificial intelligence challenge our notions of responsibility. 77.22%\n",
            " - The basketball team’s defense strategy turned the match in their favor. 75.25%\n",
            "\n",
            " The opposition party rallied against cuts to public education spending.\n",
            " - The senator proposed a bill to reform healthcare funding nationwide. 83.87%\n",
            " - Voters turned out in record numbers to support the new tax initiative. 83.71%\n",
            " - The prime minister faced criticism for delaying the climate change vote. 80.21%\n",
            "\n",
            " Antiviral drugs are being tested to shorten the duration of flu symptoms.\n"
          ]
        }
      ]
    }
  ]
}