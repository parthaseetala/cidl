{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Encoder-only LLM to build a Multi-Task Model (Classification + Labeling)**\n",
        "\n",
        "**Author: Partha Seetala**\n",
        "\n",
        "Video Tutorial: [https://youtu.be/UJZ4HGLnSMU](https://youtu.be/UJZ4HGLnSMU)"
      ],
      "metadata": {
        "id": "ud_rSqnAmyLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Install HuggingFace Transformers Library**"
      ],
      "metadata": {
        "id": "Ahi1qBV_0IIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers datasets torchinfo scikit-learn\n",
        "!pip install \"numpy<2.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4pOhhuGYgSc",
        "outputId": "0b94b268-a2c3-4ad7-e7cb-eabc1d75f9b9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchinfo, fsspec, scikit-learn, transformers, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.7.0\n",
            "    Uninstalling fsspec-2025.7.0:\n",
            "      Successfully uninstalled fsspec-2025.7.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.53.2\n",
            "    Uninstalling transformers-4.53.2:\n",
            "      Successfully uninstalled transformers-4.53.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.7.0 requires fsspec==2025.7.0, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0 scikit-learn-1.7.1 torchinfo-1.8.0 transformers-4.53.3\n",
            "Collecting numpy<2.0\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "32fb70eec27d4b688647d2679c4f329c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Import required modules**"
      ],
      "metadata": {
        "id": "rIK2TD3t0PCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"   # disable W&B prompts\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "id": "xDX9wO5RYrC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Load Google Drive into Colab to load training dataset and cached models**"
      ],
      "metadata": {
        "id": "JTwupj830feP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "USECASE_NAME = \"s3e2-classification-and-labeling\"\n",
        "\n",
        "logmsg_data = \"s3e2-linux-log-messages.csv\"\n",
        "k8smsg_data = \"s3e2-k8s-log-messages.csv\"\n",
        "\n",
        "DATASET_TYPE = \"kubernetes\" # \"kubernetes\" | \"logmsg\"\n",
        "\n",
        "if DATASET_TYPE == \"logmsg\":\n",
        "    selected_dataset = logmsg_data\n",
        "elif DATASET_TYPE == \"kubernetes\":\n",
        "    selected_dataset = k8smsg_data\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dataset type: {DATASET_TYPE}\")\n",
        "\n",
        "DATASET_NAME = Path(selected_dataset).stem\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "max_seq_len = 128\n",
        "\n",
        "ROOTDIR = '/content/drive/MyDrive/cidl'\n",
        "\n",
        "HF_TOKEN_FILEPATH = os.path.join(ROOTDIR, \"hf.token\")\n",
        "DATASET_DIR = os.path.join(ROOTDIR, 'datasets')  # ~/cidl/datasets\n",
        "PRETRAINED_MODEL_DIR = os.path.join(ROOTDIR, 'models', 'pretrained', 'bert')  # ~/cidl/models/pretrained/bert\n",
        "FINETUNED_MODEL_DIR = os.path.join(ROOTDIR, 'models', 'finetuned', 'bert', USECASE_NAME, DATASET_NAME)  # ~/cidl/models/finetuned/bert/<usecase-name>/<dataset-name>\n",
        "\n",
        "def load_hugging_face_token(hf_token_filepath=HF_TOKEN_FILEPATH):\n",
        "    if os.path.exists(hf_token_filepath):\n",
        "        with open(hf_token_filepath, \"r\") as f:\n",
        "            hf_token = f.read().strip()\n",
        "\n",
        "        if hf_token.startswith(\"hf_\"):\n",
        "            # Login and set environment variable\n",
        "            os.environ[\"HF_TOKEN\"] = hf_token\n",
        "            #login(token=hf_token, add_to_git_credential=False)\n",
        "        else:\n",
        "            print(f\"Malformed Hugging Face token file at: {hf_token_filepath}\")\n",
        "    else:\n",
        "        print(f\"Hugging Face token file not found at: {hf_token_filepath}\")\n",
        "\n",
        "for dirpath in [DATASET_DIR, PRETRAINED_MODEL_DIR, FINETUNED_MODEL_DIR]:\n",
        "    os.makedirs(dirpath, exist_ok=True)\n",
        "\n",
        "load_hugging_face_token(HF_TOKEN_FILEPATH)\n",
        "\n",
        "print(\"Dataset directory .................................... \", DATASET_DIR)\n",
        "print(\"Location where pretrained model will be downloaded ... \", PRETRAINED_MODEL_DIR)\n",
        "print(\"Location where finetuned model will be stored ........ \", FINETUNED_MODEL_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ktjkvr1Y5Td",
        "outputId": "4ef5dfd6-f359-4f86-d4aa-310930f0a2d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset directory ....................................  /content/drive/MyDrive/cidl/datasets\n",
            "Location where pretrained model will be downloaded ...  /content/drive/MyDrive/cidl/models/pretrained/bert\n",
            "Location where finetuned model will be stored ........  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-classification-and-labeling/s3e2-k8s-log-messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 1: Build our custom Classificaton + Labeling Model**"
      ],
      "metadata": {
        "id": "MHG2rqCUUax3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download/Load Pre-trained Models**"
      ],
      "metadata": {
        "id": "5pnC8Qtb8HkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_pretrained_encoder_only_model(model_dir, model_name):\n",
        "    # Download the pre-trained Encoder-only Transformer model and the pre-trained Tokenizer for that model\n",
        "    AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)\n",
        "    AutoModel.from_pretrained(model_name, cache_dir=model_dir)\n",
        "\n",
        "print(\"Downloading pretrained Encoder-only LLM model '{}' into '{}\".format(MODEL_NAME, PRETRAINED_MODEL_DIR))\n",
        "download_pretrained_encoder_only_model(model_dir=PRETRAINED_MODEL_DIR, model_name=MODEL_NAME)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qZST8NVx8Rot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d00098e-bdff-47af-d43d-c4135a472529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pretrained Encoder-only LLM model 'bert-base-uncased' into '/content/drive/MyDrive/cidl/models/pretrained/bert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of our custom Classificaiton + Label MultiTask Model and other utility functions**"
      ],
      "metadata": {
        "id": "aBn5YDnBOHTG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassificationLabelingModel(torch.nn.Module):\n",
        "    def __init__(self, model_name, cache_dir, label_names, device=None):\n",
        "        super().__init__()\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.num_labels = len(label_names)\n",
        "        self.label_names = label_names\n",
        "\n",
        "        self.base_model = AutoModel.from_pretrained(pretrained_model_name_or_path=model_name, cache_dir=cache_dir)\n",
        "        self.base_model = self.base_model.to(device)\n",
        "\n",
        "        # Seq Len = S (512)\n",
        "        # Dimensions = D (768)\n",
        "        # [SxD]\n",
        "\n",
        "        hidden_size = self.base_model.config.hidden_size  # D\n",
        "\n",
        "        # Urgency FFN head --> Urgent/Normal Classification\n",
        "        self.urgency_classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size, hidden_size),   # [DxD]\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_size, 2) # [Dx2]\n",
        "        ).to(device)\n",
        "        self.urgency_loss_fn = torch.nn.CrossEntropyLoss()  # Softmax()\n",
        "\n",
        "        # Multilabel FFN head -- Multi-label Head\n",
        "        self.multilabel_classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size, hidden_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_size, self.num_labels)  # [DxL]\n",
        "        ).to(device)\n",
        "        self.multilabel_loss_fn = torch.nn.BCEWithLogitsLoss() # -> Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, urgency_label=None, multilabel_targets=None):\n",
        "        # \"Pod is sitting in a crashloop\" => [CLS, pod, is, sitting, in, a, crashloop, PAD, PAD]\n",
        "        # input_ids => [1, 23, 45, 67, 99, 3, 17, 100, 100]\n",
        "        # atten_mask=> [0, 1, 1, 1, 1, 1, 1, 0, 0]\n",
        "\n",
        "        output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)  # [SxD]\n",
        "        cls_output = output.last_hidden_state[:, 0]  # [CLS] token representation\n",
        "\n",
        "        # CLS -> [1xD]\n",
        "\n",
        "        # Urgency Classifier FFN2\n",
        "        #   [1xD]*[DxD] => [1xD] -> ReLU() -> [1xD]*[Dx2] => [1x2]  [{0=urgent},{1=normal}]\n",
        "        urgency_logits = self.urgency_classifier(cls_output)  # Ypred_urgency_classifier\n",
        "\n",
        "        # Multi-labeler FFN2\n",
        "        #  [1xD]*[DxD] => [1xD] -> ReLU() -> [1xD]*[DxL] => [1xL]\n",
        "        multilabel_logits = self.multilabel_classifier(cls_output)  # Ypred_multilabeler\n",
        "\n",
        "        if urgency_label is not None and multilabel_targets is not None:\n",
        "            # We are training here\n",
        "            loss_urgency    = self.urgency_loss_fn(urgency_logits, urgency_label)\n",
        "            loss_multilabel = self.multilabel_loss_fn(multilabel_logits, multilabel_targets.float())\n",
        "            loss = loss_urgency + loss_multilabel\n",
        "        else:\n",
        "            # We are running inference here\n",
        "            loss = None\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"urgency_logits\": urgency_logits,\n",
        "            \"multilabel_logits\": multilabel_logits\n",
        "        }\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "        # Save updated weights of Base Encoder-only Transformer Model\n",
        "        self.base_model.save_pretrained(save_directory)\n",
        "\n",
        "        # Save weights of custom classifier heads\n",
        "        torch.save({\n",
        "            \"urgency_classifier\": self.urgency_classifier.state_dict(),\n",
        "            \"multilabel_classifier\": self.multilabel_classifier.state_dict()\n",
        "        }, os.path.join(save_directory, \"custom_classifier_heads.bin\"))\n",
        "\n",
        "        # Hugging Face-style config\n",
        "        hf_config = AutoConfig.from_pretrained(self.model_name)\n",
        "        hf_config.model_name = self.model_name\n",
        "        hf_config.num_labels = self.num_labels\n",
        "        hf_config.label_names = self.label_names\n",
        "        hf_config.save_pretrained(save_directory)\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, load_directory):\n",
        "        # Load config\n",
        "        hf_config = AutoConfig.from_pretrained(load_directory)\n",
        "\n",
        "        # Initialize model\n",
        "        model = cls(\n",
        "            model_name=hf_config._name_or_path,\n",
        "            cache_dir=None,\n",
        "            label_names=hf_config.label_names)\n",
        "\n",
        "        model.base_model = AutoModel.from_pretrained(load_directory)\n",
        "\n",
        "        # Load custom classifier weights\n",
        "        head_weights = torch.load(os.path.join(load_directory, \"custom_classifier_heads.bin\"), map_location=\"cpu\")\n",
        "        model.urgency_classifier.load_state_dict(head_weights[\"urgency_classifier\"])\n",
        "        model.multilabel_classifier.load_state_dict(head_weights[\"multilabel_classifier\"])\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        config = {\n",
        "            'model_name': hf_config._name_or_path,\n",
        "            'num_labels': hf_config.num_labels,\n",
        "            'label_names': hf_config.label_names\n",
        "        }\n",
        "\n",
        "        return model, config\n",
        "\n",
        "def display_classification_and_labeling_model_summary(model):\n",
        "    print(\"\\n{:<60} {:<20} {:>15} {:>12}\".format(\"Layer (type)\", \"Shape (S×D)\", \"Param #\", \"Trainable\"))\n",
        "    print(\"=\" * 115)\n",
        "\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    total_bytes = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        shape = \" × \".join(map(str, param.shape))\n",
        "        num_params = param.numel()\n",
        "        size_bytes = param.element_size() * num_params\n",
        "        total_params += num_params\n",
        "        total_bytes += size_bytes\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "        print(f\"{name:<60} {shape:<20} {num_params:>15,} {str(param.requires_grad):>12}\")\n",
        "\n",
        "    print(\"=\" * 115)\n",
        "    print(f\"Total Parameters         : {total_params:,}\")\n",
        "    print(f\"Trainable Parameters     : {trainable_params:,}\")\n",
        "    print(f\"Non-trainable Parameters : {total_params - trainable_params:,}\")\n",
        "    print(f\"Model Size (approx)      : {total_bytes / (1024**2):.2f} MiB  ({total_bytes / (1024**3):.2f} GiB)\")\n",
        "\n",
        "\n",
        "def build_classification_and_labeling_model(model_dir, model_name, label_names, device=None):\n",
        "    return ClassificationLabelingModel(model_name=model_name, cache_dir=model_dir, label_names=label_names, device=device)\n",
        "\n",
        "def finetune_classification_and_labeling_model(model, model_dir, train_dataset, val_dataset, epochs=3):\n",
        "    def collate_fn(batch):\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch], dtype=torch.long),\n",
        "            \"urgency_label\": torch.tensor([x[\"urgency_label\"] for x in batch], dtype=torch.long),\n",
        "            \"multilabel_targets\": torch.tensor([x[\"multilabel_targets\"] for x in batch], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "    output_dir = os.path.join(model_dir, \"checkpoints\")\n",
        "    log_dir    = os.path.join(model_dir, \"logs\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=epochs,\n",
        "        logging_steps=10,\n",
        "        logging_dir=log_dir,\n",
        "        save_total_limit=min(epochs, 3),\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=collate_fn\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "def save_classification_and_labeling_model(tokenizer, model, finetuned_model_dir):\n",
        "    print(\"Saving finetuned model to dir: \", finetuned_model_dir)\n",
        "    model.save_pretrained(finetuned_model_dir)\n",
        "    tokenizer.save_pretrained(finetuned_model_dir)\n",
        "\n",
        "def load_classification_and_labeling_model(finetuned_model_dir):\n",
        "    print(\"Loading model from:\", finetuned_model_dir)\n",
        "    model, config = ClassificationLabelingModel.from_pretrained(finetuned_model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n",
        "    return tokenizer, model, config\n",
        "\n",
        "def classify_and_label_support_issue(tokenizer, model, sentence, label_names):\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"]\n",
        "        )\n",
        "    urgency = torch.argmax(F.softmax(outputs[\"urgency_logits\"], dim=1)).item()\n",
        "    multilabel_probs = torch.sigmoid(outputs[\"multilabel_logits\"]).squeeze().tolist()\n",
        "    if isinstance(multilabel_probs, float):\n",
        "        multilabel_probs = [multilabel_probs]\n",
        "    labels = [label for label, prob in zip(label_names, multilabel_probs) if prob >= 0.5]\n",
        "    return (\"Urgent\" if urgency == 1 else \"Normal\"), labels\n"
      ],
      "metadata": {
        "id": "RwD46kIO5qvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 2: Finetune our custom model by training with task-specific data**"
      ],
      "metadata": {
        "id": "kxoTjF-QAGvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Dataset for fine-tuning model**"
      ],
      "metadata": {
        "id": "x5H6z1E65a_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset_for_finetuning(dataset_csv, tokenizer, max_seqlen=128):\n",
        "    # FAST PATH: check if we have a previously pre-processed dataset instead of parsing CSV again\n",
        "    preprocessed_dir = os.path.splitext(dataset_csv)[0] + \"-preprocessed\"\n",
        "    dataset_cache_file = os.path.join(preprocessed_dir, \"dataset.ds\")\n",
        "    labels_file = os.path.join(preprocessed_dir, \"labels.bin\")\n",
        "\n",
        "    # FAST PATH: Check if preprocessed files exist\n",
        "    if all(os.path.exists(f) for f in [dataset_cache_file, labels_file]):\n",
        "        dataset = load_from_disk(dataset_cache_file)\n",
        "        with open(labels_file, \"r\") as f:\n",
        "            labels = json.load(f)\n",
        "        train_test = dataset.train_test_split(test_size=0.1)\n",
        "        print(\"Loaded pre-processed dataset from prior cache at {}\".format(preprocessed_dir))\n",
        "        return train_test[\"train\"], train_test[\"test\"], labels\n",
        "\n",
        "    df = pd.read_csv(dataset_csv)\n",
        "\n",
        "    # Make all column names lowercase for case-insensitive access\n",
        "    df.columns = df.columns.str.lower()\n",
        "\n",
        "    # Ensure 'text', 'urgency', and 'labels' columns exist\n",
        "    required_cols = {\"text\", \"urgency\", \"labels\"}\n",
        "    missing_cols = required_cols - set(df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing required columns in CSV: {missing_cols}\")\n",
        "\n",
        "    # Normalize column values\n",
        "    df[\"text\"] = df[\"text\"].astype(str)\n",
        "    df[\"urgency\"] = df[\"urgency\"].astype(str).str.lower().str.strip()\n",
        "    df[\"urgency_label\"] = df[\"urgency\"].astype(str).str.strip().str.lower().map({\"normal\": 0, \"urgent\": 1})\n",
        "\n",
        "    # Drop rows where urgency_label could not be mapped\n",
        "    df = df[df[\"urgency_label\"].notnull()]\n",
        "    df[\"urgency_label\"] = df[\"urgency_label\"].astype(int)\n",
        "\n",
        "    # parse labels column\n",
        "    df['labels'] = df['labels'].apply(eval)\n",
        "\n",
        "    # text1 -> labels=[\"tag1\", \"tag2\"] -> [1, 1, 0]\n",
        "    # text2 -> labels=[\"tag3\"] -> [0, 0, 1]\n",
        "    # text3 -> labels=[\"tag2\", \"tag3\"] -> [0, 1, 1]\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    multilabel = mlb.fit_transform(df['labels'])  # text1 -> [1, 1, 0] | text2 -> [0, 0, 1] | text3 -> [0, 1, 1]\n",
        "    label_names = list(mlb.classes_)  # [\"tag1\", \"tag2\", \"tag3\"]\n",
        "    num_labels = len(label_names)     # 3\n",
        "\n",
        "    multilabel_df = pd.DataFrame(multilabel, columns=[f\"label_{l}\" for l in label_names])\n",
        "    df = pd.concat([df.reset_index(drop=True), multilabel_df], axis=1)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        df['text'].tolist(),\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_seqlen,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"urgency_label\": df[\"urgency_label\"].tolist(),\n",
        "        \"multilabel_targets\": multilabel.tolist()\n",
        "    })\n",
        "\n",
        "    # SAVE in preprocessed cache for future use\n",
        "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "    dataset.save_to_disk(dataset_cache_file)\n",
        "    with open(labels_file, \"w\") as f:\n",
        "        json.dump(label_names, f)\n",
        "\n",
        "    train_test = dataset.train_test_split(test_size=0.1)\n",
        "    return train_test[\"train\"], train_test[\"test\"], label_names\n",
        "\n",
        "DATASET_FILE_PATH = os.path.join(DATASET_DIR, selected_dataset)\n",
        "\n",
        "print(\"Loading training dataset from: \", DATASET_FILE_PATH)\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=PRETRAINED_MODEL_DIR)\n",
        "\n",
        "train_dataset, val_dataset, label_names = prepare_dataset_for_finetuning(dataset_csv=DATASET_FILE_PATH, tokenizer=tokenizer, max_seqlen=max_seq_len)\n",
        "\n",
        "print(\"# messages for training: {}\".format(len(train_dataset)))\n",
        "print(\"# messages for validation: {}\".format(len(val_dataset)))\n",
        "print(\"unique labels: ({}) {}\".format(len(label_names), label_names))"
      ],
      "metadata": {
        "id": "HyuR7VtnwNld",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b1fdec-cb57-4c10-b589-36ae82182bee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training dataset from:  /content/drive/MyDrive/cidl/datasets/s3e2-k8s-log-messages.csv\n",
            "Loaded pre-processed dataset from prior cache at /content/drive/MyDrive/cidl/datasets/s3e2-k8s-log-messages-preprocessed\n",
            "# messages for training: 45018\n",
            "# messages for validation: 5002\n",
            "unique labels: (30) ['API-Server', 'Auth', 'Business-Impact', 'ConfigMap', 'Configuration', 'Container-Registry', 'DaemonSet', 'Database', 'Deployment', 'Deployment-Issues', 'Development', 'Infrastructure', 'Ingress', 'Networking', 'Node-Health', 'PVC', 'Performance', 'Pod', 'RBAC', 'Resource-Issues', 'Resource-Management', 'Scaling', 'Scheduling', 'Secret', 'Security', 'Service', 'Service-Discovery', 'StatefulSet', 'Storage', 'Traffic-Management']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tune Model using this dataset**"
      ],
      "metadata": {
        "id": "Us1Pb001-GY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_classification_and_labeling_model(model_dir=PRETRAINED_MODEL_DIR, model_name=MODEL_NAME, label_names=label_names)\n",
        "\n",
        "finetune_classification_and_labeling_model(model=model, model_dir=FINETUNED_MODEL_DIR, train_dataset=train_dataset, val_dataset=val_dataset, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT8V-x9g0VxV",
        "outputId": "32d952a9-635a-4898-cdf9-a3f5dcaa5f82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8442' max='8442' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8442/8442 12:55, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000555</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000052</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Show summary of model architecture**"
      ],
      "metadata": {
        "id": "5vYBGMAul-Oc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_classification_and_labeling_model_summary(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HDqQ4YzCmBWm",
        "outputId": "6b176885-ace6-4475-ef1c-66bcfc31cbeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Layer (type)                                                 Shape (S×D)                  Param #    Trainable\n",
            "===================================================================================================================\n",
            "base_model.embeddings.word_embeddings.weight                 30522 × 768               23,440,896         True\n",
            "base_model.embeddings.position_embeddings.weight             512 × 768                    393,216         True\n",
            "base_model.embeddings.token_type_embeddings.weight           2 × 768                        1,536         True\n",
            "base_model.embeddings.LayerNorm.weight                       768                              768         True\n",
            "base_model.embeddings.LayerNorm.bias                         768                              768         True\n",
            "base_model.encoder.layer.0.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.0.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.0.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.0.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.0.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.0.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.0.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.0.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.0.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.0.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.0.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.0.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.0.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.0.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.0.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.0.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.1.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.1.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.1.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.1.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.1.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.1.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.1.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.1.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.1.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.1.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.1.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.1.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.1.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.1.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.1.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.1.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.2.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.2.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.2.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.2.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.2.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.2.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.2.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.2.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.2.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.2.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.2.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.2.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.2.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.2.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.2.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.2.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.3.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.3.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.3.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.3.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.3.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.3.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.3.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.3.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.3.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.3.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.3.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.3.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.3.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.3.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.3.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.3.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.4.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.4.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.4.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.4.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.4.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.4.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.4.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.4.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.4.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.4.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.4.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.4.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.4.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.4.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.4.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.4.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.5.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.5.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.5.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.5.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.5.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.5.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.5.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.5.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.5.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.5.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.5.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.5.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.5.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.5.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.5.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.5.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.6.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.6.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.6.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.6.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.6.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.6.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.6.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.6.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.6.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.6.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.6.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.6.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.6.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.6.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.6.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.6.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.7.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.7.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.7.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.7.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.7.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.7.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.7.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.7.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.7.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.7.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.7.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.7.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.7.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.7.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.7.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.7.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.8.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.8.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.8.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.8.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.8.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.8.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.8.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.8.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.8.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.8.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.8.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.8.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.8.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.8.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.8.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.8.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.9.attention.self.query.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.9.attention.self.query.bias         768                              768         True\n",
            "base_model.encoder.layer.9.attention.self.key.weight         768 × 768                    589,824         True\n",
            "base_model.encoder.layer.9.attention.self.key.bias           768                              768         True\n",
            "base_model.encoder.layer.9.attention.self.value.weight       768 × 768                    589,824         True\n",
            "base_model.encoder.layer.9.attention.self.value.bias         768                              768         True\n",
            "base_model.encoder.layer.9.attention.output.dense.weight     768 × 768                    589,824         True\n",
            "base_model.encoder.layer.9.attention.output.dense.bias       768                              768         True\n",
            "base_model.encoder.layer.9.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.9.attention.output.LayerNorm.bias   768                              768         True\n",
            "base_model.encoder.layer.9.intermediate.dense.weight         3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.9.intermediate.dense.bias           3072                           3,072         True\n",
            "base_model.encoder.layer.9.output.dense.weight               768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.9.output.dense.bias                 768                              768         True\n",
            "base_model.encoder.layer.9.output.LayerNorm.weight           768                              768         True\n",
            "base_model.encoder.layer.9.output.LayerNorm.bias             768                              768         True\n",
            "base_model.encoder.layer.10.attention.self.query.weight      768 × 768                    589,824         True\n",
            "base_model.encoder.layer.10.attention.self.query.bias        768                              768         True\n",
            "base_model.encoder.layer.10.attention.self.key.weight        768 × 768                    589,824         True\n",
            "base_model.encoder.layer.10.attention.self.key.bias          768                              768         True\n",
            "base_model.encoder.layer.10.attention.self.value.weight      768 × 768                    589,824         True\n",
            "base_model.encoder.layer.10.attention.self.value.bias        768                              768         True\n",
            "base_model.encoder.layer.10.attention.output.dense.weight    768 × 768                    589,824         True\n",
            "base_model.encoder.layer.10.attention.output.dense.bias      768                              768         True\n",
            "base_model.encoder.layer.10.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.10.attention.output.LayerNorm.bias  768                              768         True\n",
            "base_model.encoder.layer.10.intermediate.dense.weight        3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.10.intermediate.dense.bias          3072                           3,072         True\n",
            "base_model.encoder.layer.10.output.dense.weight              768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.10.output.dense.bias                768                              768         True\n",
            "base_model.encoder.layer.10.output.LayerNorm.weight          768                              768         True\n",
            "base_model.encoder.layer.10.output.LayerNorm.bias            768                              768         True\n",
            "base_model.encoder.layer.11.attention.self.query.weight      768 × 768                    589,824         True\n",
            "base_model.encoder.layer.11.attention.self.query.bias        768                              768         True\n",
            "base_model.encoder.layer.11.attention.self.key.weight        768 × 768                    589,824         True\n",
            "base_model.encoder.layer.11.attention.self.key.bias          768                              768         True\n",
            "base_model.encoder.layer.11.attention.self.value.weight      768 × 768                    589,824         True\n",
            "base_model.encoder.layer.11.attention.self.value.bias        768                              768         True\n",
            "base_model.encoder.layer.11.attention.output.dense.weight    768 × 768                    589,824         True\n",
            "base_model.encoder.layer.11.attention.output.dense.bias      768                              768         True\n",
            "base_model.encoder.layer.11.attention.output.LayerNorm.weight 768                              768         True\n",
            "base_model.encoder.layer.11.attention.output.LayerNorm.bias  768                              768         True\n",
            "base_model.encoder.layer.11.intermediate.dense.weight        3072 × 768                 2,359,296         True\n",
            "base_model.encoder.layer.11.intermediate.dense.bias          3072                           3,072         True\n",
            "base_model.encoder.layer.11.output.dense.weight              768 × 3072                 2,359,296         True\n",
            "base_model.encoder.layer.11.output.dense.bias                768                              768         True\n",
            "base_model.encoder.layer.11.output.LayerNorm.weight          768                              768         True\n",
            "base_model.encoder.layer.11.output.LayerNorm.bias            768                              768         True\n",
            "base_model.pooler.dense.weight                               768 × 768                    589,824         True\n",
            "base_model.pooler.dense.bias                                 768                              768         True\n",
            "urgency_classifier.0.weight                                  768 × 768                    589,824         True\n",
            "urgency_classifier.0.bias                                    768                              768         True\n",
            "urgency_classifier.2.weight                                  2 × 768                        1,536         True\n",
            "urgency_classifier.2.bias                                    2                                  2         True\n",
            "multilabel_classifier.0.weight                               768 × 768                    589,824         True\n",
            "multilabel_classifier.0.bias                                 768                              768         True\n",
            "multilabel_classifier.2.weight                               30 × 768                      23,040         True\n",
            "multilabel_classifier.2.bias                                 30                                30         True\n",
            "===================================================================================================================\n",
            "Total Parameters         : 110,688,032\n",
            "Trainable Parameters     : 110,688,032\n",
            "Non-trainable Parameters : 0\n",
            "Model Size (approx)      : 422.24 MiB  (0.41 GiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save out fine-tuned Model**"
      ],
      "metadata": {
        "id": "8ezUlKXgPpPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_classification_and_labeling_model(tokenizer, model, FINETUNED_MODEL_DIR)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "ae90b9e7ac2443618979e9fe4b7fcd52",
            "7e821317041a446f8cbb8e9f64a43c64",
            "281b4cae43424b1aae73b635fd786869",
            "dbf280c85e024c4a95d9d481ce40c99d",
            "6b85a859433a4897bbf74f9842600589",
            "5994b798f1604e5698a708d43b00e0b9",
            "0fff839d81e54f3790f96874c79246f8",
            "a9b3a48ff8f84ab391461c5d7262ba57",
            "cff66c337217458a91ba9bbe49249d38",
            "72c82537fcb544b0aa0b5dd24e8bda8d",
            "160f45551fac471e99fb5dcf30009816"
          ]
        },
        "id": "2PAIWFPIFuyU",
        "outputId": "565e3ff5-b653-43c4-bf27-2dd8edd7a99e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving finetuned model to dir:  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-classification-and-labeling/s3e2-k8s-log-messages\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae90b9e7ac2443618979e9fe4b7fcd52"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 3: Use fine-tuned custom model to classify and label support issues**"
      ],
      "metadata": {
        "id": "vre4xJzF_9VL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "dataset_type = \"kubernetes\"\n",
        "\n",
        "if dataset_type == \"logmsg\":\n",
        "    issues = [\n",
        "        \"The VPN connection keeps timing out and users are unable to login.\",\n",
        "        \"Routine filesystem scan completed successfully.\",\n",
        "        \"Previously corrupted filesystem scan completed successfully.\",\n",
        "        \"hey I tried to mount a file system at /dev/foobar, however, it fails me everytime\",\n",
        "        \"/dev/barfoo can't access it! what should I do?\",\n",
        "    ]\n",
        "elif dataset_type == \"kubernetes\":\n",
        "    issues = [\n",
        "        'Back-off restarting failed container',\n",
        "        'CrashLoopBackOff: Container failed to start repeatedly',\n",
        "        'Error: ImagePullBackOff',\n",
        "        'Failed to pull image \"nginx:latest\": rpc error: code = Unknown desc = failed to resolve image',\n",
        "        'container \"app\" terminated with exit code 137',\n",
        "        'Liveness probe failed: HTTP probe failed with statuscode: 503',\n",
        "        'Readiness probe failed: dial tcp 10.0.0.42:8080: connect: connection refused',\n",
        "        'ReplicaSet \"webapp-5f8657f9b6\" has timed out progressing',\n",
        "        'Deployment \"api\" exceeded its progress deadline',\n",
        "        'FailedCreate: Error creating: pods \"web-xyz-\" is forbidden: exceeded quota',\n",
        "        'no endpoints available for service \"my-backend\"',\n",
        "        'Failed to resolve DNS name: kube-dns.kube-system.svc.cluster.local',\n",
        "        'NetworkPolicy denied ingress traffic to pod \"web-123\"',\n",
        "        'Failed to attach volume \"pvc-1234abcd\" to node \"node-1\": timeout while waiting for mount',\n",
        "        'MountVolume.MountDevice failed for volume \"data\": rpc error: code = Internal desc = mount failed',\n",
        "        'pod has unbound immediate PersistentVolumeClaims',\n",
        "        'Node \"gke-node-123\" not ready: NetworkUnavailable',\n",
        "        '0/5 nodes are available: 2 Insufficient cpu, 1 Insufficient memory',\n",
        "        'Pod eviction due to node pressure: memory pressure',\n",
        "        'User \"system:serviceaccount:default:my-sa\" is forbidden: User cannot list resource \"pods\" in API group \"\" in the namespace \"default\"'\n",
        "    ]\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dataset type: {dataset_type}\")\n",
        "\n",
        "\n",
        "tokenizer, model, config = load_classification_and_labeling_model(FINETUNED_MODEL_DIR)\n",
        "label_names = config['label_names']\n"
      ],
      "metadata": {
        "id": "vspZRketjKau",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f19fb83-6f3e-411b-ae58-cb4b20d977f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-classification-and-labeling/s3e2-k8s-log-messages\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for issue in issues:\n",
        "    start_time = time.time()\n",
        "\n",
        "    urgency, labels = classify_and_label_support_issue(tokenizer, model, issue, label_names=label_names)\n",
        "\n",
        "    elapsed_ms = round((time.time() - start_time) * 1000)\n",
        "\n",
        "    urgency_str = f\"\\033[91mUrgent\\033[0m\" if urgency == \"Urgent\" else urgency # show urgent messages in RED\n",
        "\n",
        "    print(f\"ISSUE: {issue}\\n → Time: {elapsed_ms} ms\\n → Urgency: {urgency_str}\\n → Labels: {labels}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n29jWtiE4QBX",
        "outputId": "8bc85160-50e3-42a1-a503-acf61ba16f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ISSUE: Back-off restarting failed container\n",
            " → Time: 309 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Node-Health', 'Pod', 'Resource-Issues']\n",
            "\n",
            "ISSUE: CrashLoopBackOff: Container failed to start repeatedly\n",
            " → Time: 45 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Node-Health', 'Pod', 'Resource-Issues']\n",
            "\n",
            "ISSUE: Error: ImagePullBackOff\n",
            " → Time: 27 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Configuration', 'Container-Registry', 'Pod']\n",
            "\n",
            "ISSUE: Failed to pull image \"nginx:latest\": rpc error: code = Unknown desc = failed to resolve image\n",
            " → Time: 54 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Configuration', 'Container-Registry', 'Pod']\n",
            "\n",
            "ISSUE: container \"app\" terminated with exit code 137\n",
            " → Time: 28 ms\n",
            " → Urgency: Normal\n",
            " → Labels: ['Pod', 'Resource-Issues']\n",
            "\n",
            "ISSUE: Liveness probe failed: HTTP probe failed with statuscode: 503\n",
            " → Time: 47 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Configuration', 'Scheduling']\n",
            "\n",
            "ISSUE: Readiness probe failed: dial tcp 10.0.0.42:8080: connect: connection refused\n",
            " → Time: 49 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Auth', 'Networking']\n",
            "\n",
            "ISSUE: ReplicaSet \"webapp-5f8657f9b6\" has timed out progressing\n",
            " → Time: 52 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['DaemonSet', 'Deployment-Issues', 'Scaling']\n",
            "\n",
            "ISSUE: Deployment \"api\" exceeded its progress deadline\n",
            " → Time: 42 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Deployment', 'Deployment-Issues', 'Scaling']\n",
            "\n",
            "ISSUE: FailedCreate: Error creating: pods \"web-xyz-\" is forbidden: exceeded quota\n",
            " → Time: 67 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: []\n",
            "\n",
            "ISSUE: no endpoints available for service \"my-backend\"\n",
            " → Time: 32 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Networking', 'Service', 'Service-Discovery']\n",
            "\n",
            "ISSUE: Failed to resolve DNS name: kube-dns.kube-system.svc.cluster.local\n",
            " → Time: 49 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: []\n",
            "\n",
            "ISSUE: NetworkPolicy denied ingress traffic to pod \"web-123\"\n",
            " → Time: 48 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Ingress', 'Networking', 'Service-Discovery']\n",
            "\n",
            "ISSUE: Failed to attach volume \"pvc-1234abcd\" to node \"node-1\": timeout while waiting for mount\n",
            " → Time: 64 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Infrastructure', 'PVC', 'Performance', 'Storage']\n",
            "\n",
            "ISSUE: MountVolume.MountDevice failed for volume \"data\": rpc error: code = Internal desc = mount failed\n",
            " → Time: 51 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Infrastructure', 'PVC', 'Storage']\n",
            "\n",
            "ISSUE: pod has unbound immediate PersistentVolumeClaims\n",
            " → Time: 33 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Deployment-Issues']\n",
            "\n",
            "ISSUE: Node \"gke-node-123\" not ready: NetworkUnavailable\n",
            " → Time: 46 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['DaemonSet', 'Networking']\n",
            "\n",
            "ISSUE: 0/5 nodes are available: 2 Insufficient cpu, 1 Insufficient memory\n",
            " → Time: 45 ms\n",
            " → Urgency: \u001b[91mUrgent\u001b[0m\n",
            " → Labels: ['Resource-Management']\n",
            "\n",
            "ISSUE: Pod eviction due to node pressure: memory pressure\n",
            " → Time: 29 ms\n",
            " → Urgency: Normal\n",
            " → Labels: ['Node-Health', 'Pod', 'Resource-Issues', 'Resource-Management']\n",
            "\n",
            "ISSUE: User \"system:serviceaccount:default:my-sa\" is forbidden: User cannot list resource \"pods\" in API group \"\" in the namespace \"default\"\n",
            " → Time: 55 ms\n",
            " → Urgency: Normal\n",
            " → Labels: []\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
