{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Encoder-only LLM to perform Named Entity Extraction**\n",
        "\n",
        "**Author: Partha Seetala**\n",
        "\n",
        "Video Tutorial: [https://youtu.be/UJZ4HGLnSMU](https://youtu.be/UJZ4HGLnSMU)"
      ],
      "metadata": {
        "id": "KqCLA5JUkNWt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import required modules**"
      ],
      "metadata": {
        "id": "HkCaivEtE1NS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW2reQFbr-vz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_MODE\"] = \"offline\"   # disable W&B prompts\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "from datasets import load_dataset\n",
        "from datasets import load_from_disk\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "import random\n",
        "import json\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load Google Drive into Colab to load and save our models**"
      ],
      "metadata": {
        "id": "jzCVB_vXE5Z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "USECASE_NAME = \"s3e2-extract-entities\"\n",
        "\n",
        "mit_ner_data = \"mit-ner-dataset.csv\"\n",
        "wikipedia_ner_data = \"wikipedia-ner-dataset.tsv\"\n",
        "\n",
        "DATASET_TYPE = \"wikipedia\" # \"mit\" | \"wikipedia\"\n",
        "\n",
        "if DATASET_TYPE == \"mit\":\n",
        "    selected_dataset = mit_ner_data\n",
        "elif DATASET_TYPE == \"wikipedia\":\n",
        "    selected_dataset = wikipedia_ner_data\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dataset type: {DATASET_TYPE}\")\n",
        "\n",
        "DATASET_NAME = Path(selected_dataset).stem\n",
        "\n",
        "MODEL_NAME = \"google/electra-base-discriminator\" # \"bert-base-uncased\"\n",
        "max_seq_len = 128\n",
        "\n",
        "ROOTDIR = '/content/drive/MyDrive/cidl'\n",
        "\n",
        "HF_TOKEN_FILEPATH = os.path.join(ROOTDIR, \"hf.token\")\n",
        "DATASET_DIR = os.path.join(ROOTDIR, 'datasets')  # ~/cidl/datasets\n",
        "PRETRAINED_MODEL_DIR = os.path.join(ROOTDIR, 'models', 'pretrained', 'bert')  # ~/cidl/models/pretrained/bert\n",
        "FINETUNED_MODEL_DIR = os.path.join(ROOTDIR, 'models', 'finetuned', 'bert', USECASE_NAME, DATASET_NAME)  # ~/cidl/models/finetuned/bert/<usecase-name>/<dataset-name>\n",
        "\n",
        "def load_hugging_face_token(hf_token_filepath=HF_TOKEN_FILEPATH):\n",
        "    if os.path.exists(hf_token_filepath):\n",
        "        with open(hf_token_filepath, \"r\") as f:\n",
        "            hf_token = f.read().strip()\n",
        "\n",
        "        if hf_token.startswith(\"hf_\"):\n",
        "            # Login and set environment variable\n",
        "            os.environ[\"HF_TOKEN\"] = hf_token\n",
        "            #login(token=hf_token, add_to_git_credential=False)\n",
        "        else:\n",
        "            print(f\"Malformed Hugging Face token file at: {hf_token_filepath}\")\n",
        "    else:\n",
        "        print(f\"Hugging Face token file not found at: {hf_token_filepath}\")\n",
        "\n",
        "for dirpath in [DATASET_DIR, PRETRAINED_MODEL_DIR, FINETUNED_MODEL_DIR]:\n",
        "    os.makedirs(dirpath, exist_ok=True)\n",
        "\n",
        "load_hugging_face_token(HF_TOKEN_FILEPATH)\n",
        "\n",
        "print(\"Dataset directory .................................... \", DATASET_DIR)\n",
        "print(\"Location where pretrained model will be downloaded ... \", PRETRAINED_MODEL_DIR)\n",
        "print(\"Location where finetuned model will be stored ........ \", FINETUNED_MODEL_DIR)"
      ],
      "metadata": {
        "id": "z8uU7pFysLzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c20816b6-b255-4a54-f87c-1555a051e5d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset directory ....................................  /content/drive/MyDrive/cidl/datasets\n",
            "Location where pretrained model will be downloaded ...  /content/drive/MyDrive/cidl/models/pretrained/bert\n",
            "Location where finetuned model will be stored ........  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-extract-entities/wikipedia-ner-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **STEP 1: Build our custom Entity Extraction Model**"
      ],
      "metadata": {
        "id": "cQws55lcFEcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ExtractEntityModel(torch.nn.Module):\n",
        "    def __init__(self, model_name, cache_dir, num_ner_tags, device=None):\n",
        "        super().__init__()\n",
        "        if device is None:\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.device = device\n",
        "\n",
        "        from transformers import AutoModel\n",
        "        self.model_name = model_name\n",
        "        self.num_ner_tags = num_ner_tags  # N\n",
        "\n",
        "        self.base_model = AutoModel.from_pretrained(model_name, cache_dir=cache_dir)\n",
        "        self.base_model = self.base_model.to(device)\n",
        "\n",
        "        # SxD\n",
        "\n",
        "        hidden_size = self.base_model.config.hidden_size  # D\n",
        "\n",
        "        # [SxD] -> FFN  (DxD) -> ReLU() -> [DxN]\n",
        "        # [SxD]*[DxD] => [SxD] -> ReLU() -> [SxD]*[DxN] => [SxN]\n",
        "        self.ner_classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_size, hidden_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(hidden_size, num_ner_tags)\n",
        "        ).to(device)\n",
        "        self.ner_loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
        "        # John lives in New York -> [CLS, John, lives, in, New, York, PAD, PAD] ->\n",
        "        # input_ids=[5000, 23, 43, 8, 90, 89, 5001, 5001]\n",
        "        # attn_mask=[1, 1, 1, 1, 1, 1, 0, 0]\n",
        "        output = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden = output.last_hidden_state  # [SxD]\n",
        "        ner_logits = self.ner_classifier(last_hidden)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.ner_loss_fn(ner_logits.view(-1, ner_logits.shape[-1]), labels.view(-1))\n",
        "\n",
        "        return {\n",
        "            \"loss\": loss,\n",
        "            \"logits\": ner_logits\n",
        "        }\n",
        "\n",
        "    def save_pretrained(self, save_directory, tag2id=None, id2tag=None):\n",
        "        import os\n",
        "        from transformers import AutoConfig\n",
        "        os.makedirs(save_directory, exist_ok=True)\n",
        "        self.base_model.save_pretrained(save_directory)\n",
        "        torch.save({\n",
        "            \"ner_classifier\": self.ner_classifier.state_dict()\n",
        "        }, os.path.join(save_directory, \"custom_classifier_heads.bin\"))\n",
        "\n",
        "        hf_config = AutoConfig.from_pretrained(self.model_name)\n",
        "        hf_config.model_name = self.model_name\n",
        "        hf_config.num_ner_tags = self.num_ner_tags\n",
        "        hf_config.save_pretrained(save_directory)\n",
        "\n",
        "        if tag2id and id2tag:\n",
        "            with open(os.path.join(save_directory, \"ner_tag2id.json\"), \"w\") as f:\n",
        "                d = {str(k): int(v) for k, v in tag2id.items()}\n",
        "                print(d)\n",
        "                json.dump({str(k): int(v) for k, v in tag2id.items()}, f)\n",
        "            with open(os.path.join(save_directory, \"ner_id2tag.json\"), \"w\") as f:\n",
        "                d = {str(k): v for k, v in id2tag.items()}\n",
        "                json.dump({str(k): v for k, v in id2tag.items()}, f)\n",
        "\n",
        "        check_files = [\"custom_classifier_heads.bin\", \"ner_tag2id.json\", \"ner_id2tag.json\"]\n",
        "        for f in check_files:\n",
        "            if not os.path.exists(os.path.join(save_directory, f)):\n",
        "                raise ValueError(f\"Missing file: {f}\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, load_directory):\n",
        "        from transformers import AutoConfig, AutoModel\n",
        "        import torch\n",
        "        hf_config = AutoConfig.from_pretrained(load_directory)\n",
        "        model = cls(\n",
        "            model_name=hf_config.model_name,\n",
        "            cache_dir=None,\n",
        "            num_ner_tags=hf_config.num_ner_tags)\n",
        "\n",
        "        model.base_model = AutoModel.from_pretrained(load_directory)\n",
        "        head_weights = torch.load(os.path.join(load_directory, \"custom_classifier_heads.bin\"), map_location=\"cpu\")\n",
        "        model.ner_classifier.load_state_dict(head_weights[\"ner_classifier\"])\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with open(os.path.join(load_directory, \"ner_id2tag.json\")) as f:\n",
        "            id2tag = {int(k): v for k, v in json.load(f).items()}\n",
        "\n",
        "        with open(os.path.join(load_directory, \"ner_tag2id.json\")) as f:\n",
        "            tag2id = json.load(f)\n",
        "\n",
        "        return model, {\n",
        "            \"model_name\": hf_config.model_name,\n",
        "            \"num_ner_tags\": hf_config.num_ner_tags,\n",
        "            \"id2tag\": id2tag,\n",
        "            \"tag2id\": tag2id\n",
        "        }\n",
        "\n",
        "def download_pretrained_encoder_only_model(model_dir, model_name):\n",
        "    # Download the pre-trained Encoder-only Transformer model and the pre-trained Tokenizer for that model\n",
        "    AutoTokenizer.from_pretrained(model_name, cache_dir=model_dir)\n",
        "    AutoModel.from_pretrained(model_name, cache_dir=model_dir)\n",
        "\n",
        "def build_entity_extraction_model(model_dir, model_name, num_ner_tags):\n",
        "    return ExtractEntityModel(model_name=model_name, cache_dir=model_dir, num_ner_tags=num_ner_tags)\n",
        "\n",
        "def finetune_entity_extraction_model(model, model_dir, train_dataset, val_dataset, epochs=3):\n",
        "    def collate_fn(batch):\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch], dtype=torch.long),\n",
        "            \"labels\": torch.tensor([x[\"ner_tags\"] for x in batch], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    output_dir = os.path.join(model_dir, \"checkpoints\")\n",
        "    log_dir    = os.path.join(model_dir, \"logs\")\n",
        "\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        run_name=\"extract-entities\",\n",
        "        output_dir=output_dir,\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        num_train_epochs=epochs,\n",
        "        logging_steps=10,\n",
        "        logging_dir=log_dir,\n",
        "        save_total_limit=min(epochs, 3),\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"eval_loss\",\n",
        "        remove_unused_columns=False\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        data_collator=collate_fn\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "def display_entity_extraction_model(model):\n",
        "    print(\"\\n{:<60} {:<20} {:>15} {:>12}\".format(\"Layer (type)\", \"Shape (S×D)\", \"Param #\", \"Trainable\"))\n",
        "    print(\"=\" * 115)\n",
        "\n",
        "    total_params = 0\n",
        "    trainable_params = 0\n",
        "    total_bytes = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        shape = \" × \".join(map(str, param.shape))\n",
        "        num_params = param.numel()\n",
        "        size_bytes = param.element_size() * num_params\n",
        "        total_params += num_params\n",
        "        total_bytes += size_bytes\n",
        "        if param.requires_grad:\n",
        "            trainable_params += num_params\n",
        "        print(f\"{name:<60} {shape:<20} {num_params:>15,} {str(param.requires_grad):>12}\")\n",
        "\n",
        "    print(\"=\" * 115)\n",
        "    print(f\"Total Parameters         : {total_params:,}\")\n",
        "    print(f\"Trainable Parameters     : {trainable_params:,}\")\n",
        "    print(f\"Non-trainable Parameters : {total_params - trainable_params:,}\")\n",
        "    print(f\"Model Size (approx)      : {total_bytes / (1024**2):.2f} MiB  ({total_bytes / (1024**3):.2f} GiB)\")\n",
        "\n",
        "def save_finetuned_entity_extraction_model(tokenizer, model, finetuned_model_dir, tags2id, id2tags):\n",
        "    print(\"Saving finetuned model to dir: \", finetuned_model_dir)\n",
        "    model.save_pretrained(finetuned_model_dir, tags2id, id2tags)\n",
        "    tokenizer.save_pretrained(finetuned_model_dir)\n",
        "\n",
        "def load_finetuned_entity_extraction_model(finetuned_model_dir):\n",
        "    print(\"Loading model from:\", finetuned_model_dir)\n",
        "    model, config = ExtractEntityModel.from_pretrained(finetuned_model_dir)\n",
        "    tokenizer = AutoTokenizer.from_pretrained(finetuned_model_dir)\n",
        "    return tokenizer, model, config\n",
        "\n",
        "def extract_entities_from_text(tokenizer, model, text, id2tag, max_seqlen=128):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input text\n",
        "    encoded = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_seqlen,\n",
        "        return_offsets_mapping=True,\n",
        "        return_token_type_ids=False\n",
        "    )\n",
        "\n",
        "    offset_mapping = encoded.pop(\"offset_mapping\")[0].tolist()\n",
        "    word_ids = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=max_seqlen\n",
        "    ).word_ids()\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in encoded.items()}\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    logits = outputs[\"logits\"]\n",
        "    predictions = torch.argmax(logits, dim=-1).squeeze().tolist()\n",
        "    tags = [id2tag.get(p, \"O\") for p in predictions]\n",
        "\n",
        "    # Step 1: collect raw entity spans\n",
        "    raw_entities = []\n",
        "    current_type = None\n",
        "    current_start = None\n",
        "    current_end = None\n",
        "\n",
        "    for tag, (start, end), word_idx in zip(tags, offset_mapping, word_ids):\n",
        "        if word_idx is None or start == end:\n",
        "            continue\n",
        "\n",
        "        if tag == \"O\":\n",
        "            if current_type:\n",
        "                raw_entities.append({\n",
        "                    \"entity\": text[current_start:current_end],\n",
        "                    \"type\": current_type,\n",
        "                    \"start\": current_start,\n",
        "                    \"end\": current_end\n",
        "                })\n",
        "                current_type = None\n",
        "            continue\n",
        "\n",
        "        prefix, label = tag.split(\"-\", 1)\n",
        "\n",
        "        if prefix == \"B\":\n",
        "            if current_type:\n",
        "                raw_entities.append({\n",
        "                    \"entity\": text[current_start:current_end],\n",
        "                    \"type\": current_type,\n",
        "                    \"start\": current_start,\n",
        "                    \"end\": current_end\n",
        "                })\n",
        "            current_type = label\n",
        "            current_start = start\n",
        "            current_end = end\n",
        "        elif prefix == \"I\" and current_type == label:\n",
        "            current_end = end\n",
        "        else:\n",
        "            if current_type:\n",
        "                raw_entities.append({\n",
        "                    \"entity\": text[current_start:current_end],\n",
        "                    \"type\": current_type,\n",
        "                    \"start\": current_start,\n",
        "                    \"end\": current_end\n",
        "                })\n",
        "            current_type = None\n",
        "\n",
        "    if current_type:\n",
        "        raw_entities.append({\n",
        "            \"entity\": text[current_start:current_end],\n",
        "            \"type\": current_type,\n",
        "            \"start\": current_start,\n",
        "            \"end\": current_end\n",
        "        })\n",
        "\n",
        "    # Step 2: merge adjacent tokens of the same type (excluding 'O')\n",
        "    grouped = {}\n",
        "    prev_type = None\n",
        "    buffer = []\n",
        "\n",
        "    def flush():\n",
        "        nonlocal buffer, prev_type\n",
        "        if buffer and prev_type and prev_type != \"O\":\n",
        "            phrase = \" \".join(buffer)\n",
        "            if prev_type not in grouped:\n",
        "                grouped[prev_type] = []\n",
        "            grouped[prev_type].append(phrase)\n",
        "        buffer = []\n",
        "\n",
        "    for ent in raw_entities:\n",
        "        ent_type = ent[\"type\"]\n",
        "        ent_text = ent[\"entity\"].strip()\n",
        "\n",
        "        if ent_type == prev_type:\n",
        "            buffer.append(ent_text)\n",
        "        else:\n",
        "            flush()\n",
        "            buffer = [ent_text]\n",
        "            prev_type = ent_type\n",
        "\n",
        "    flush()\n",
        "    return grouped\n"
      ],
      "metadata": {
        "id": "1UCPJkydsBiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading pretrained Encoder-only LLM model '{}' into '{}\".format(MODEL_NAME, PRETRAINED_MODEL_DIR))\n",
        "download_pretrained_encoder_only_model(model_dir=PRETRAINED_MODEL_DIR, model_name=MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lcm7uzM_1F6H",
        "outputId": "758c3f45-3f45-4766-c70a-b7b620837b38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading pretrained Encoder-only LLM model 'google/electra-base-discriminator' into '/content/drive/MyDrive/cidl/models/pretrained/bert\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**STEP 2: Finetune our custom model by training with task-specific training data**"
      ],
      "metadata": {
        "id": "D-fHof2cFMIh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare Dataset for finetuning our model**"
      ],
      "metadata": {
        "id": "X0MdBimfFXJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def parse_tsv_fileformat(filepath):\n",
        "    def build_sentence_and_bio_entities(tokens, tags):\n",
        "        text = \"\"\n",
        "        entities = []\n",
        "\n",
        "        for token, tag in zip(tokens, tags):\n",
        "            if text:\n",
        "                text += \" \"\n",
        "            start = len(text)\n",
        "            text += token\n",
        "            end = len(text)\n",
        "\n",
        "            entities.append({\n",
        "                \"text\": token,\n",
        "                \"type\": tag,\n",
        "                \"start\": start,\n",
        "                \"end\": end\n",
        "            })\n",
        "\n",
        "        return text, entities\n",
        "\n",
        "    sentences = []\n",
        "    tokens = []\n",
        "    tags = []\n",
        "\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        count = 0\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            if not line:\n",
        "                if tokens:\n",
        "                    text, entities = build_sentence_and_bio_entities(tokens, tags)\n",
        "                    sentences.append({\"text\": text, \"entities\": entities})\n",
        "                    tokens = []\n",
        "                    tags = []\n",
        "                continue\n",
        "\n",
        "            parts = line.split(\"\\t\")\n",
        "            if len(parts) < 3:\n",
        "                continue\n",
        "\n",
        "            token = parts[1]\n",
        "            tag = parts[2]\n",
        "            tokens.append(token)\n",
        "            tags.append(tag)\n",
        "\n",
        "        # Final flush\n",
        "        if tokens:\n",
        "            text, entities = build_sentence_and_bio_entities(tokens, tags)\n",
        "            sentences.append({\"text\": text, \"entities\": entities})\n",
        "\n",
        "    return pd.DataFrame(sentences)\n",
        "\n",
        "\n",
        "\n",
        "def parse_csv_fileformat(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    return df\n",
        "\n",
        "\n",
        "def prepare_dataset_for_finetuning(dataset_filepath, tokenizer, max_seqlen=128):\n",
        "\n",
        "    # preprocessed directory path\n",
        "    preprocessed_dir = os.path.splitext(dataset_filepath)[0] + \"-preprocessed\"\n",
        "    dataset_cache_file = os.path.join(preprocessed_dir, \"dataset.ds\")\n",
        "    id2tag_file = os.path.join(preprocessed_dir, \"id2tag.bin\")\n",
        "    tag2id_file = os.path.join(preprocessed_dir, \"tag2id.bin\")\n",
        "\n",
        "    # FAST PATH: Check if preprocessed files exist\n",
        "    if all(os.path.exists(f) for f in [dataset_cache_file, id2tag_file, tag2id_file]):\n",
        "        dataset = load_from_disk(dataset_cache_file)\n",
        "        with open(id2tag_file, \"r\") as f:\n",
        "            id2tag = json.load(f)\n",
        "            id2tag = {int(k): v for k, v in id2tag.items()}\n",
        "        with open(tag2id_file, \"r\") as f:\n",
        "            tag2id = json.load(f)\n",
        "        train_test = dataset.train_test_split(test_size=0.1)\n",
        "        return train_test[\"train\"], train_test[\"test\"], tag2id, id2tag\n",
        "\n",
        "    # SLOW PATH: build the dataframe\n",
        "    if dataset_filepath.endswith(\".tsv\"):\n",
        "        df = parse_tsv_fileformat(dataset_filepath)\n",
        "    elif dataset_filepath.endswith(\".csv\"):\n",
        "        df = parse_csv_fileformat(dataset_filepath)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown file format: {dataset_filepath}\")\n",
        "\n",
        "    df.columns = df.columns.str.lower()\n",
        "    df[\"text\"] = df[\"text\"].astype(str)\n",
        "    if dataset_filepath.endswith(\".csv\"):\n",
        "        df[\"entities\"] = df[\"entities\"].apply(lambda x: ast.literal_eval(x) if pd.notna(x) and isinstance(x, str) and x.strip() else [])\n",
        "\n",
        "    texts = df[\"text\"].tolist()\n",
        "    all_word_lists = []\n",
        "    all_word_labels = []\n",
        "\n",
        "    tag_set = {\"O\"}\n",
        "\n",
        "\n",
        "    # I live in California ->\n",
        "    # [I, live, in, California]\n",
        "    # [O, O,    O,  B_LOC]\n",
        "    # [I, live, in, Cali, #fornia]\n",
        "    # [O, O,    O,  B_LOC, I_LOC]\n",
        "\n",
        "    for text, entities in zip(df[\"text\"], df[\"entities\"]):\n",
        "        words = text.split()\n",
        "        word_starts = []\n",
        "        pos = 0\n",
        "        for word in words:\n",
        "            start = text.find(word, pos)\n",
        "            word_starts.append(start)\n",
        "            pos = start + len(word)\n",
        "\n",
        "        word_tags = [\"O\"] * len(words)\n",
        "        for ent in entities:\n",
        "            ent_start = ent[\"start\"]\n",
        "            ent_end = ent[\"end\"]\n",
        "            ent_type = ent[\"type\"].split(\"-\")[-1]\n",
        "\n",
        "            for i, start in enumerate(word_starts):\n",
        "                end = start + len(words[i])\n",
        "                if start == ent_start:\n",
        "                    word_tags[i] = f\"B-{ent_type}\"\n",
        "                elif ent_start < start < ent_end:\n",
        "                    word_tags[i] = f\"I-{ent_type}\"\n",
        "\n",
        "        all_word_lists.append(words)\n",
        "        all_word_labels.append(word_tags)\n",
        "        tag_set.update(word_tags)\n",
        "\n",
        "    for tag in list(tag_set):\n",
        "        if tag.startswith(\"B-\"):\n",
        "            i_tag = tag.replace(\"B-\", \"I-\")\n",
        "            tag_set.add(i_tag)\n",
        "\n",
        "    tag_list = sorted(tag_set)\n",
        "    tag2id = {tag: i for i, tag in enumerate(tag_list)}\n",
        "    id2tag = {i: tag for tag, i in tag2id.items()}\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        all_word_lists,\n",
        "        is_split_into_words=True,\n",
        "        return_offsets_mapping=False,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=max_seqlen\n",
        "    )\n",
        "\n",
        "    all_labels = []\n",
        "\n",
        "    for i, word_ids in enumerate(tokenized.word_ids(batch_index=i) for i in range(len(tokenized[\"input_ids\"]))):\n",
        "        word_tags = all_word_labels[i]\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(tag2id[word_tags[word_idx]])\n",
        "            else:\n",
        "                # same word as previous token → propagate I-*\n",
        "                label = word_tags[word_idx]\n",
        "                if label.startswith(\"B-\"):\n",
        "                    label = label.replace(\"B-\", \"I-\")\n",
        "                label_ids.append(tag2id[label])\n",
        "            previous_word_idx = word_idx\n",
        "        all_labels.append(label_ids)\n",
        "\n",
        "    dataset = Dataset.from_dict({\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"ner_tags\": all_labels\n",
        "    })\n",
        "\n",
        "    # SAVE in preprocessed cache for future use\n",
        "    os.makedirs(preprocessed_dir, exist_ok=True)\n",
        "    dataset.save_to_disk(dataset_cache_file)\n",
        "    with open(id2tag_file, \"w\") as f:\n",
        "        json.dump({str(k): v for k, v in id2tag.items()}, f)\n",
        "    with open(tag2id_file, \"w\") as f:\n",
        "        json.dump(tag2id, f)\n",
        "\n",
        "    train_test = dataset.train_test_split(test_size=0.1)\n",
        "    return train_test[\"train\"], train_test[\"test\"], tag2id, id2tag\n",
        "\n",
        "\n",
        "\n",
        "DATASET_FILE_PATH = os.path.join(DATASET_DIR, selected_dataset)\n",
        "\n",
        "print(\"Loading training dataset from: \", DATASET_FILE_PATH)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=PRETRAINED_MODEL_DIR)\n",
        "\n",
        "train_dataset, val_dataset, tags2id, id2tags = prepare_dataset_for_finetuning(dataset_filepath=DATASET_FILE_PATH, tokenizer=tokenizer, max_seqlen=max_seq_len)\n",
        "\n",
        "print(\"number of messages for training: {}\".format(len(train_dataset)))\n",
        "print(\"number of messages for validation: {}\".format(len(val_dataset)))\n",
        "print(\"number of tags {} {}: \".format(len(tags2id), tags2id))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv18AsjD1KWn",
        "outputId": "02e996f6-a45f-4033-e4b9-03822ef0d681",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training dataset from:  /content/drive/MyDrive/cidl/datasets/wikipedia-ner-dataset.tsv\n",
            "number of messages for training: 118152\n",
            "number of messages for validation: 13128\n",
            "number of tags 33 {'B-ANIM': 0, 'B-BIO': 1, 'B-CEL': 2, 'B-DIS': 3, 'B-EVE': 4, 'B-FOOD': 5, 'B-INST': 6, 'B-LOC': 7, 'B-MEDIA': 8, 'B-MYTH': 9, 'B-O': 10, 'B-ORG': 11, 'B-PER': 12, 'B-PLANT': 13, 'B-TIME': 14, 'B-VEHI': 15, 'I-ANIM': 16, 'I-BIO': 17, 'I-CEL': 18, 'I-DIS': 19, 'I-EVE': 20, 'I-FOOD': 21, 'I-INST': 22, 'I-LOC': 23, 'I-MEDIA': 24, 'I-MYTH': 25, 'I-O': 26, 'I-ORG': 27, 'I-PER': 28, 'I-PLANT': 29, 'I-TIME': 30, 'I-VEHI': 31, 'O': 32}: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the model and run the finetuning training run**"
      ],
      "metadata": {
        "id": "9HIhZbMiFc8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_entity_extraction_model(model_dir=PRETRAINED_MODEL_DIR, model_name=MODEL_NAME, num_ner_tags=len(tags2id))\n",
        "\n",
        "finetune_entity_extraction_model(model=model, model_dir=FINETUNED_MODEL_DIR, train_dataset=train_dataset, val_dataset=val_dataset, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "c3dmmB2d3kUg",
        "outputId": "74dde2af-47ca-49c0-8845-08b2d37ce5f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22155' max='22155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22155/22155 37:10, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.040500</td>\n",
              "      <td>0.064871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>0.057069</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.034500</td>\n",
              "      <td>0.060788</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save our finetuned model for future use**"
      ],
      "metadata": {
        "id": "afC2HlseFhs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_finetuned_entity_extraction_model(tokenizer, model, FINETUNED_MODEL_DIR, tags2id, id2tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "634ad233404d4faeb89c49d8bd286bad",
            "25c859641fb84647889faf175761f21a",
            "270847bd21554989bc7476bc04d5883b",
            "4d8a25cbb4f44c5eb894ca49ffbdfe96",
            "d80516ab8a8a4ba3ada50b1cb8afabd6",
            "99a622cf25bb4bf08658342051b84eee",
            "faed714ba71545878de976b01f52ce6d",
            "ec30010071304c15a83f8dba0cb5fcea",
            "d7586d5f71504a1aa52ad120151d5267",
            "edbc2b1528354a8f9c194d56b55291ce",
            "63f1a62e420c42518922d766f638408f"
          ]
        },
        "id": "rI7eQxGw4W1a",
        "outputId": "5b25cf16-d5c9-4b8b-d5f4-8157fc839335",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving finetuned model to dir:  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-extract-entities/wikipedia-ner-dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "634ad233404d4faeb89c49d8bd286bad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'B-ANIM': 0, 'B-BIO': 1, 'B-CEL': 2, 'B-DIS': 3, 'B-EVE': 4, 'B-FOOD': 5, 'B-INST': 6, 'B-LOC': 7, 'B-MEDIA': 8, 'B-MYTH': 9, 'B-O': 10, 'B-ORG': 11, 'B-PER': 12, 'B-PLANT': 13, 'B-TIME': 14, 'B-VEHI': 15, 'I-ANIM': 16, 'I-BIO': 17, 'I-CEL': 18, 'I-DIS': 19, 'I-EVE': 20, 'I-FOOD': 21, 'I-INST': 22, 'I-LOC': 23, 'I-MEDIA': 24, 'I-MYTH': 25, 'I-O': 26, 'I-ORG': 27, 'I-PER': 28, 'I-PLANT': 29, 'I-TIME': 30, 'I-VEHI': 31, 'O': 32}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**STEP 3: Use our finetuned custom model to extract entities from text**"
      ],
      "metadata": {
        "id": "DQd0MtvaFmxd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load our previously finetuned Entity Extraction Model**"
      ],
      "metadata": {
        "id": "dFtXu3EfFrtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer, model, config = load_finetuned_entity_extraction_model(FINETUNED_MODEL_DIR)\n",
        "id2tag = config[\"id2tag\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131,
          "referenced_widgets": [
            "789bb6a9538f44308402e06060e56d8a",
            "201ff431869341bdb0704398bdae6a14",
            "fba430ce0bca4463bc0fcbced593ec8b",
            "aa15031f0d264f7eb91651674f68298c",
            "7c162008f1e74712bef3f1542e39725b",
            "f48d705590cf4ff599c8ce69663c6afc",
            "515a103b0e8646678099eeeb12c03852",
            "413ad68583fc429cbf3491d4e58bc5e0",
            "90283b4da16b4022966bb74439e09004",
            "63c03b1a0ada4b94841f24d0a8747cf7",
            "96afde9966504c658ff4c25d45c00270",
            "43d4df0e515b4571936b6029ed18855c",
            "07c002526047499cb5d227501b4fb069",
            "f739cfc6b910461fadc177598c9b1c03",
            "4e2c7ab5a2b04c60bd5ac6f272d98e04",
            "ffe8c46e3c8e4efd8b2c5fe781d25f36",
            "c623ce67188a4ffa97d28aec6cde5e64",
            "46220af931054993b405b8b88c47f279",
            "92d58648233d44d8a93161be87970cbd",
            "14e6c1eb5b4947e39e9d066010e976b8",
            "2eee17aa29df4f258c91a3c111043985",
            "8718f4beec094cca84ea0ae979f15b2e",
            "589edd919bc345aa848a2e16a5399642",
            "b5aa59effa3540358b6b79588773f5d4",
            "61c13cc0f67a42c0a192c691263f711c",
            "1b929a720b76442c8664795d8d97569b",
            "d33a274546354be988e83bf5bab4f1a8",
            "86d74f50f2624b3d8fc29baf03693f77",
            "fa72eddcce2942e8b46973bcccc62fd5",
            "fe237ef319cf43d5bae7b35f54d1f1b1",
            "f8798c5582ef4068b9800f63e8a4cdae",
            "3523eb25883649e295d55a3b8cd5e48b",
            "d0c218b9b85f42f599114b366098a2bb"
          ]
        },
        "id": "G7hkl_mFxQ42",
        "outputId": "9dd8d285-6a0d-405f-a515-bd2055bf6d36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-extract-entities/wikipedia-ner-dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/666 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "789bb6a9538f44308402e06060e56d8a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43d4df0e515b4571936b6029ed18855c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "589edd919bc345aa848a2e16a5399642"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pass text and extract entities**"
      ],
      "metadata": {
        "id": "8DrYS4YmFw4h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "sentences = [\n",
        "    \"Capital of India is New Delhi just like Paris is for France and Washington DC is for USA\",\n",
        "    \"Tom Hanks acted in the movie Cast Away, in which he was lost at sea while on a Fedex plane\",\n",
        "    \"Steph Curry plays for the Golden State Warriors, a basketball team in United States, which was earlier owned by Vivek Ranadive\",\n",
        "    \"Virat Kholi, Roger Federer, Lebron James, are legends who represented Indian Cricket Team, Swiss National Tennis Team and LA Lakers\",\n",
        "    \"Partha Seetala, who lives in Los Altos Hills, flew to meet  Sheik Abul-bin Sultan-el Hakimul Khan, who works for The Emirates Group\",\n",
        "]\n",
        "\n",
        "total_ms = 0\n",
        "for text in sentences:\n",
        "    start_time = time.time()\n",
        "\n",
        "    entities = extract_entities_from_text(tokenizer, model, text, id2tag)\n",
        "\n",
        "    elapsed_ms = round((time.time() - start_time) * 1000)\n",
        "    total_ms += elapsed_ms\n",
        "\n",
        "    print(f\"TEXT: {text}\\n → Time: {elapsed_ms} ms\\n → Entities: {entities}\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx3MCSOW4ddB",
        "outputId": "ab3244dc-48c8-4adb-873c-3f792781b269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT: Capital of India is New Delhi just like Paris is for France and Washington DC is for USA\n",
            " → Time: 5045 ms\n",
            " → Entities: {'LOC': ['India', 'New Delhi', 'Paris', 'France', 'Washington DC', 'USA']}\n",
            "\n",
            "TEXT: Tom Hanks acted in the movie Cast Away, in which he was lost at sea while on a Fedex plane\n",
            " → Time: 14 ms\n",
            " → Entities: {'PER': ['Tom Hanks'], 'ORG': ['Fedex']}\n",
            "\n",
            "TEXT: Steph Curry plays for the Golden State Warriors, a basketball team in United States, which was earlier owned by Vivek Ranadive\n",
            " → Time: 14 ms\n",
            " → Entities: {'PER': ['Steph Curry', 'Vivek Ranadive'], 'ORG': ['Golden State Warriors'], 'LOC': ['United States']}\n",
            "\n",
            "TEXT: Virat Kholi, Roger Federer, Lebron James, are legends who represented Indian Cricket Team, Swiss National Tennis Team and LA Lakers\n",
            " → Time: 13 ms\n",
            " → Entities: {'PER': ['Virat Kholi', 'Roger Federer', 'Lebron James'], 'ORG': ['Indian Cricket Team', 'Swiss National Tennis Team', 'LA Lakers']}\n",
            "\n",
            "TEXT: Partha Seetala, who lives in Los Altos Hills, flew to meet  Sheik Abul-bin Sultan-el Hakimul Khan, who works for The Emirates Group\n",
            " → Time: 13 ms\n",
            " → Entities: {'PER': ['Partha Seetala', 'She Abul-bin Sultan-el Hakimul Khan'], 'LOC': ['Los Altos Hills'], 'ORG': ['Emirates Group']}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bonus example: download text from URL and extract entities from there**"
      ],
      "metadata": {
        "id": "80HY0CwaFzu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities_from_long_text_in_chunks(tokenizer, model, text, id2tag, max_seqlen=128, stride=64):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Tokenize entire input to get full tokens + offsets\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        return_offsets_mapping=True,\n",
        "        return_token_type_ids=False,\n",
        "        return_attention_mask=False,\n",
        "        truncation=False,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    input_ids = encoding[\"input_ids\"]\n",
        "    offsets = encoding[\"offset_mapping\"]\n",
        "    total_len = len(input_ids)\n",
        "\n",
        "    grouped_entities = defaultdict(list)\n",
        "\n",
        "    for start in range(0, total_len, stride):\n",
        "        end = min(start + max_seqlen, total_len)\n",
        "\n",
        "        # Use the character span for this token slice\n",
        "        start_char = offsets[start][0]\n",
        "        end_char = offsets[end - 1][1]\n",
        "\n",
        "        chunk_text = text[start_char:end_char]\n",
        "\n",
        "        # Call your existing extract_entities function\n",
        "        chunk_entities = extract_entities_from_text(tokenizer, model, chunk_text, id2tag, max_seqlen=max_seqlen)\n",
        "\n",
        "        # Merge entities, skip duplicates\n",
        "        for ent_type, values in chunk_entities.items():\n",
        "            for val in values:\n",
        "                if val not in grouped_entities[ent_type]:\n",
        "                    grouped_entities[ent_type].append(val)\n",
        "\n",
        "        if end == total_len:\n",
        "            break  # Finished last chunk\n",
        "\n",
        "    return dict(grouped_entities)\n"
      ],
      "metadata": {
        "id": "omtwipyODP8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        response.raise_for_status()  # raise error for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "        # Remove script and style tags\n",
        "        for tag in soup([\"script\", \"style\", \"noscript\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        # Get plain text\n",
        "        text = soup.get_text(separator=\"\\n\")\n",
        "\n",
        "        # Clean up: remove extra blank lines and whitespace\n",
        "        lines = [line.strip() for line in text.splitlines()]\n",
        "        lines = [line for line in lines if line]  # remove empty lines\n",
        "        clean_text = \"\\n\".join(lines)\n",
        "\n",
        "        return clean_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching or parsing URL: {e}\"\n",
        "\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/Sam_Altman\"\n",
        "text = extract_text_from_url(url)\n",
        "\n",
        "\n",
        "entities = extract_entities_from_long_text_in_chunks(tokenizer, model, text, id2tags)\n",
        "\n",
        "for ent in entities:\n",
        "    print(ent, entities[ent])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szZP34kG9-wE",
        "outputId": "aa50f285-d550-4309-8df2-afd677efa2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (11305 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PER ['Sam Altman', 'Samuel Harris Altman', 'Sam Altman Altman', 'Oliver Mulherin', 'Patrick Chung', 'Paul Graham', 'Jack Altman', 'Elon Musk', 'Jessica Livingston', 'Peter Thiel', 'Altman', 'Greg Brockman', 'Rishi Sunak', 'Emmanuel Macron', 'Pedro Sánchez', 'Olaf Scholz', 'Narendra Modi', 'Yoon Suk-yeol', 'Isaac Herzog', 'Ursula von', 'Ursula von der Leyen', 'Helen Toner', \"Adam D'Angelo\", 'Tasha', 'Tasha McCauley', 'Ilya Sutskever', 'Brock', 'Satya Nadella', 'Brockman', \"D'Angelo\", 'Demis Hassabis', 'Dario Amodei', 'Andrew Yang', 'Joe Biden', 'Dean Phillips', 'Daniel Lurie', 'Donald Trump', 'Mark Warner', 'Yishan Wong', 'Steve Huffman', 'Michael Klein', 'Bret Taylor', 'Balaji Srinivasan', 'Nancy Pelosi', 'Ric Weiland', 'Walter Isaacson', 'Nick Sivo', 'Ann Altman', 'Connie', 'Max', 'Jack', 'Weil', 'Elizabeth', 'Mickle', 'Trip', 'Metz', 'Cade', 'Isaac', 'Mike', 'Weise', 'Karen', 'Tripp', 'Ben Thompson', 'Chapman', 'Glenn', 'Michael', 'Jacob', 'Ben', 'Ricky', 'Schneider', 'Sharon', 'Jeremy', 'Louis. ^ Chapman', 'Bachner', 'Magid', 'Ben-David', 'Tal', 'Nguyen', 'Britney', 'Hart', 'Jordan', 'Hagy', 'Paige', 'Ankeny', 'Jason', 'Seetharaman', 'Deepa', 'Jessica E', 'Sam', 'Va', 'Jessica E.', 'Clark', 'Kate', 'a b Clark', 'Garry Tan', 'Loizos', 'Chafkin', 'McGregor', 'Jena', 'Bloomberg', 'Sara', 'Currie', 'Richard', 'Sam Alt', 'Nieva', 'Sethi', 'Aman', 'Guo', 'Eileen', 'Renaldi', 'Adi', 'Robert', 'Hammond', 'George', 'Chipolina', 'Scott', 'Njanja', 'Annie', 'How', 'Howcroft', 'Sophie', 'Atkinson', 'Roth', 'Emma', 'Vynck', 'Gerrit', 'Olanoff', 'Drew', 'Chase', 'Peters', 'Jay', 'Das', 'Shanti', 'Connett', 'David', 'Warren', 'Tom', 'Alex', 'Getahun', 'Hannah', 'Lawler', 'Hayden', 'Johnson', 'Eric', 'Kara Swisher', 'Romm', 'Tony', 'Russell', 'Mel', 'Melia', 'Tindera', 'Michela', 'Varanasi', 'Lakshmi', 'Ni', 'Kenneth', 'Niemeyer', 'Schleifer', 'Theodore', 'b Schleifer', 'Freedlander', 'Bezos', 'Zuckerberg', 'Trump', 'Rodriguez', 'Salvador', 'Alt', 'Ellen Pao', 'Robison', 'Ghaffary', 'Shirin', 'Herper', 'Matthew', 'Krystal', 'Hu', 'Tong', 'Anna', 'Jeffrey', 'Dastin', 'Hiller', 'Jennifer', 'Christine', 'Ramkumar', 'Amrith', 'Sigalos', 'MacKenzie', 'Muir', 'Martha', 'Hamilton', 'Katherine', 'Massa', 'Vernal', 'Hagey', 'Berber Jin', 'Tom Dotan', 'Keach', 'Haskins', 'Caroline', 'Bernstein', 'Joseph', 'Lucas', 'Josephy', 'Matan H.', 'Ravi', 'Akshaya', 'Bajekal', 'Naina', 'Perrido', 'Billy', 'rido', 'rigo', 'Wilson', 'Alexandra', 'Gilchrist', 'Skelton', 'Charlie', 'Kissinger', 'Isaacson', 'Walter', 'Gareth', 'Manish', 'Singh', 'Tad', 'Lynn', 'Kay', 'Grace', 'Valinsky', 'Black', 'Julia', 'Joanna', 'Partridge', 'Hoskin', 'Peter', 'Hoskins', 'Greg Brockman Sarah Friar Jakub Pachocki', 'Adam D', 'Desmond', 'Nicole Seli', 'Emmett', \"Sam Altman Adam D'Angelo Sue Desmond Zico\", 'Paul Nakasone Adebayo', 'Lawrence Summers Bret Taylor', 'Reid Hoffman', 'Holden Karnofsky', 'Shivon Zilis']\n",
            "LOC ['Indonesia', 'Nederlands', 'Chicago', 'Illinois', 'U.S.', 'St. Louis', 'Missouri', 'Ladue', 'San', 'San Francisco', 'United States', 'Kenya', 'France', 'United Kingdom', 'Bavaria', 'South Korea', 'Spain', 'Portugal', 'Hong Kong', 'South Korean', 'California', 'Virginia', 'Canada', 'Altman', 'Bilderberg', 'Hawaii', 'Russian Hill', 'Napa', 'Big Sur', 'Eastern District of Missouri', 'Ankeny', 'Silicon Valley', 'New York', 'Metz', 'Jordan', 'Shirin', 'South China']\n",
            "ORG ['Wikimedia Commons', 'Stanford University', 'Oklo Inc', 'Helion Energy', 'Oklo Inc.', 'OpenAI', 'Loopt', 'Y Combinator', 'Oklo', 'John Burroughs School', 'Altman', 'Xfund', 'New Enterprise Associates', 'Sequoia Capital', 'Green Dot Corporation', 'Airbnb', 'Dropbox', 'Zenefits', 'Stripe', 'Hydrazine Capital', 'YC Continuity', 'Y Combinator Research', 'YC', 'Microsoft', 'Amazon Web', 'Amazon Web Services', 'YC Group', 'Y Combinator partners', 'Tools For Humanity', 'Humane', 'Retro Biosciences', 'Boom Technology', 'Cruise', 'General Motors', 'American', 'American fusion', 'Infosys', 'YC Research', 'TED', 'United States Senate Judiciary Subcommittee on Privacy , Technology and the Law', 'European Commission', 'Quora', 'Verge', 'WilmerHale', 'DeepMind', 'Anthropic', 'United', 'American Bridge 21st Century', 'Democratic Party', 'World Economic Forum', 'Reddit', 'TrialSpark', 'Silicon Valley', 'Silicon Valley Bank', 'Exowatt', 'AltC Acquisition Corp', 'Bloomberg', 'Minicircle', 'University of Waterloo', 'Indonesia', 'Xfu', 'Harvard University', 'Time magazine', 'Time', 'Businessweek', 'Forbes magazine', 'Tesla', 'Musk', 'Israel Defense Forces', 'X', 'New York Times', 'Wall Street Journal', 'Washington Post', 'Pippa', 'Forbes', 'of Israel', 'TechCrunch', 'Hydrazine Capital GP', 'Securities and Exchange Commission', 'University of Michigan', 'Wired', 'Worldcoin', 'BuzzFeed', 'MIT', 'Ars Technica', 'UK', 'PBS', 'San Francisco Chronicle', 'Recode', 'Business Insider', 'New York Magazine', 'Adi', 'Bloomberg News', 'AltC Acquisition Corp\\n.', 'Financial Times', 'Harvard', 'Harvard Crimson', 'Guardian', 'Simon and Schuster', 'New Yorker', 'McLaren', 'Morning', 'CNN', 'edia Commons', 'Sora Whisper GitHub Copilot Foundation', 'Stargate LLC', 'Apple', 'hCards Commons', 'Creative Commons', 'Wikimedia Foundation']\n",
            "INST ['Apple Macintosh']\n",
            "MEDIA [\"Moore 's Law for Everything\", 'Times', 'German', 'Business', 'Google Books', 'American English']\n",
            "TIME ['Out', 'inauguration']\n"
          ]
        }
      ]
    }
  ]
}
