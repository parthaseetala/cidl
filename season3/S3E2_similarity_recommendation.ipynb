{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Encoder-only LLM to do Similarity Recommendation/Duplicate Detection**\n",
        "Using Siamese Network/Sentence Transformer and FAISS (for VectorDB)\n",
        "\n",
        "**Author: Partha Seetala**\n",
        "\n",
        "Video Tutorial: [https://youtu.be/UJZ4HGLnSMU](https://youtu.be/UJZ4HGLnSMU)"
      ],
      "metadata": {
        "id": "7MZk_g7poCkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas scikit-learn torch sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ro-yFa4y2F61",
        "outputId": "2433b3e8-0962-4856-8199-49c8813ef5ba",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.7.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed faiss-cpu-1.11.0.post1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "from itertools import combinations\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import faiss\n",
        "from sentence_transformers import (\n",
        "    SentenceTransformer,\n",
        "    InputExample,\n",
        "    losses,\n",
        "    evaluation\n",
        ")"
      ],
      "metadata": {
        "id": "ftWcCEBwBVHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "USECASE_NAME = \"s3e2-similarity-recommendation\"\n",
        "\n",
        "qabugs_data = \"similarity-qabugs.csv\"\n",
        "gitbugs_data = \"similarity-gitbugs.csv\"\n",
        "quora_data =  \"similarity-quora.parquet\"\n",
        "\n",
        "DATASET_TYPE = \"quora\"\n",
        "\n",
        "if DATASET_TYPE == \"qabugs\":\n",
        "    selected_dataset = qabugs_data\n",
        "elif DATASET_TYPE == \"gitbugs\":\n",
        "    selected_dataset = gitbugs_data\n",
        "elif DATASET_TYPE == \"quora\":\n",
        "    selected_dataset = quora_data\n",
        "else:\n",
        "    raise ValueError(f\"Unknown dataset type: {DATASET_TYPE}\")\n",
        "\n",
        "DATASET_NAME = Path(selected_dataset).stem\n",
        "\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
        "max_seq_len = 128\n",
        "\n",
        "ROOTDIR = '/content/drive/MyDrive/cidl'\n",
        "\n",
        "HF_TOKEN_FILEPATH = os.path.join(ROOTDIR, \"hf.token\")\n",
        "DATASET_DIR = os.path.join(ROOTDIR, 'datasets')  # ~/cidl/datasets\n",
        "PRETRAINED_MODEL_DIR = os.path.join(ROOTDIR, 'models', 'pretrained', 'bert')  # ~/cidl/models/pretrained/bert\n",
        "FINETUNED_MODEL_DIR = os.path.join(ROOTDIR, 'models', 'finetuned', 'bert', USECASE_NAME, DATASET_NAME, \"model\")  # ~/cidl/models/finetuned/bert/<usecase-name>/<dataset-name>\n",
        "VECTORDB_DIR = os.path.join(ROOTDIR, 'models', 'finetuned', 'bert', USECASE_NAME, DATASET_NAME, \"vectordb\")\n",
        "VECTORDB_FILE = os.path.join(VECTORDB_DIR, \"vdb\")\n",
        "\n",
        "def load_hugging_face_token(hf_token_filepath=HF_TOKEN_FILEPATH):\n",
        "    if os.path.exists(hf_token_filepath):\n",
        "        with open(hf_token_filepath, \"r\") as f:\n",
        "            hf_token = f.read().strip()\n",
        "\n",
        "        if hf_token.startswith(\"hf_\"):\n",
        "            # Login and set environment variable\n",
        "            os.environ[\"HF_TOKEN\"] = hf_token\n",
        "            #login(token=hf_token, add_to_git_credential=False)\n",
        "        else:\n",
        "            print(f\"Malformed Hugging Face token file at: {hf_token_filepath}\")\n",
        "    else:\n",
        "        print(f\"Hugging Face token file not found at: {hf_token_filepath}\")\n",
        "\n",
        "for dirpath in [DATASET_DIR, PRETRAINED_MODEL_DIR, FINETUNED_MODEL_DIR, VECTORDB_DIR]:\n",
        "    os.makedirs(dirpath, exist_ok=True)\n",
        "\n",
        "load_hugging_face_token(HF_TOKEN_FILEPATH)\n",
        "\n",
        "print(\"DATASET DIR ............. \", DATASET_DIR)\n",
        "print(\"MODEL NAME .............. \", MODEL_NAME)\n",
        "print(\"PRETRAINED MODEL DIR .... \", PRETRAINED_MODEL_DIR)\n",
        "print(\"FINETUNED MODEL DIR ..... \", FINETUNED_MODEL_DIR)\n",
        "print(\"VECTORDB DIR ............ \", VECTORDB_DIR)"
      ],
      "metadata": {
        "id": "LVQiK7vWxCmz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "462a833a-9f38-4793-8eaa-f5233c6adb70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "DATASET DIR .............  /content/drive/MyDrive/cidl/datasets\n",
            "MODEL NAME ..............  all-MiniLM-L6-v2\n",
            "PRETRAINED MODEL DIR ....  /content/drive/MyDrive/cidl/models/pretrained/bert\n",
            "FINETUNED MODEL DIR .....  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-similarity-recommendation/similarity-quora/model\n",
            "VECTORDB DIR ............  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-similarity-recommendation/similarity-quora/vectordb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Prepare Training, Validation and Testing Dataset**"
      ],
      "metadata": {
        "id": "MbxmZdSTOA4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_training_and_test_data(dataset_file, dataset_type, val_size=0.1, test_size=0.1, random_state=42):\n",
        "    if dataset_type == \"qabugs\":\n",
        "        print(f\"Loading dataset from file: {dataset_file}\")\n",
        "        df = pd.read_csv(dataset_file)\n",
        "    elif dataset_type == \"gitbugs\":\n",
        "        print(f\"Loading dataset from file: {dataset_file}\")\n",
        "        df = pd.read_csv(dataset_file)\n",
        "    elif dataset_type == \"quora\":\n",
        "        # Check if the file already exists\n",
        "        if not os.path.exists(dataset_file):\n",
        "            # Download from HF and save to the dataset_file\n",
        "            hf_path = \"hf://datasets/AlekseyKorshuk/quora-question-pairs/data/train-00000-of-00001.parquet\"\n",
        "            print(f\"Downloading dataset from Hugging Face location {hf_path}...\")\n",
        "            df = pd.read_parquet(hf_path)\n",
        "            print(f\"Saving dataset to local cache: {dataset_file}\")\n",
        "            df.to_parquet(dataset_file, index=False)\n",
        "        else:\n",
        "            # Load from local cache\n",
        "            print(f\"Loading dataset from file: {dataset_file}\")\n",
        "            df = pd.read_parquet(dataset_file)\n",
        "\n",
        "        # Rename columns\n",
        "        df = df.rename(columns={\n",
        "            \"question1\":    \"text_a\",\n",
        "            \"question2\":    \"text_b\",\n",
        "            \"is_duplicate\": \"score\"\n",
        "        })\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown file format for dataset file {dataset_file}\")\n",
        "\n",
        "    df_train_val, df_test = train_test_split(df, test_size=test_size, random_state=random_state)\n",
        "    val_rel = val_size / (1 - test_size)\n",
        "    df_train, df_val = train_test_split(df_train_val, test_size=val_rel, random_state=random_state)\n",
        "\n",
        "    def to_examples(sub_df):\n",
        "        a = sub_df['text_a'].values\n",
        "        b = sub_df['text_b'].values\n",
        "        s = sub_df['score'].astype(float).values\n",
        "        return [\n",
        "            InputExample(texts=[ta, tb], label=sc)\n",
        "            for ta, tb, sc in zip(a, b, s)\n",
        "        ]\n",
        "\n",
        "    return to_examples(df_train), to_examples(df_val), to_examples(df_test)"
      ],
      "metadata": {
        "id": "omLJP0YC9p52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main Functions to Finetune and do Inference on Sentence Transformer**"
      ],
      "metadata": {
        "id": "vAIK2n4LOGhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pretrained_similarity_model(model_dir=PRETRAINED_MODEL_DIR, model_name=MODEL_NAME):\n",
        "    os.environ['SENTENCE_TRANSFORMERS_HOME'] = model_dir\n",
        "    print(\"Loading SentenceTransformer('{}')\".format(model_name))\n",
        "    model = SentenceTransformer(model_name)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "def finetune_similarity_model(model, model_dir, train, val, epochs=1, batch_size=16, lr=2e-5):\n",
        "    train_dataloader = DataLoader(train, shuffle=True, batch_size=batch_size)\n",
        "    train_loss = losses.CosineSimilarityLoss(model)\n",
        "    evaluator = evaluation.EmbeddingSimilarityEvaluator.from_input_examples(val, name=\"val-eval\")\n",
        "\n",
        "    model.fit(\n",
        "        train_objectives=[(train_dataloader, train_loss)],\n",
        "        evaluator=evaluator,\n",
        "        epochs=epochs,\n",
        "        warmup_steps=100,\n",
        "        optimizer_params={\"lr\": lr},\n",
        "        output_path=model_dir,\n",
        "        show_progress_bar=True\n",
        "    )\n",
        "\n",
        "def save_finetuned_similarity_model(model, finetuned_path):\n",
        "    model.save(finetuned_path)\n",
        "\n",
        "def load_finetuned_similarity_model(model_dir=FINETUNED_MODEL_DIR):\n",
        "    print(f\"Loading finetuned model from {model_dir}\")\n",
        "    model = SentenceTransformer(model_dir)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model\n",
        "\n",
        "def find_similar(model, vdb, sentence, topk=10, threshold=0.6):\n",
        "\n",
        "    def get_embedding(model, sentence):\n",
        "        emb = model.encode([sentence], convert_to_numpy=True)\n",
        "        faiss.normalize_L2(emb)\n",
        "        return emb[0]\n",
        "\n",
        "    embedding = get_embedding(model, sentence)\n",
        "\n",
        "    D, I = vdb[\"index\"].search(embedding.reshape(1, -1), topk)\n",
        "    results = []\n",
        "    for idx, score in zip(I[0], D[0]):\n",
        "        if threshold is not None and score < threshold:\n",
        "            break\n",
        "        text = vdb[\"texts\"][idx]\n",
        "        if text == sentence:\n",
        "            continue\n",
        "        results.append((text, float(score)))\n",
        "    return results"
      ],
      "metadata": {
        "id": "PP-K9u8JwAYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main code to Build, Save and Query VectorDB**"
      ],
      "metadata": {
        "id": "iZ5IPnAyOVlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_database(model, sentences):\n",
        "    # collect all texts\n",
        "    texts = []\n",
        "    for sent in sentences:\n",
        "        texts.extend(sent.texts)\n",
        "    # remove duplicates\n",
        "    texts = list(dict.fromkeys(texts))\n",
        "\n",
        "    # encode & normalize\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "    faiss.normalize_L2(embeddings)\n",
        "\n",
        "    # build index\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatIP(dim)  # cosine = inner product on normalized vectors\n",
        "    index.add(embeddings)\n",
        "\n",
        "    vdb = {\"index\": index, \"texts\": texts}\n",
        "    return vdb\n",
        "\n",
        "def save_similarity_database(vdb, vdbfilepath):\n",
        "    faiss.write_index(vdb[\"index\"], vdbfilepath + \".index\")\n",
        "    with open(vdbfilepath + \".texts.pkl\", \"wb\") as f:\n",
        "        pickle.dump(vdb[\"texts\"], f)\n",
        "\n",
        "\n",
        "def load_similarity_database(vdbfilepath):\n",
        "    print(\"Loading Similarity VectorDB from \", vdbfilepath)\n",
        "    index = faiss.read_index(vdbfilepath + \".index\")\n",
        "    with open(vdbfilepath + \".texts.pkl\", \"rb\") as f:\n",
        "        texts = pickle.load(f)\n",
        "    vdb = {\"index\": index, \"texts\": texts}\n",
        "    return vdb\n"
      ],
      "metadata": {
        "id": "sBlgL6lMOQ27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Finetuning an already pre-trained SentenceTransformer**"
      ],
      "metadata": {
        "id": "ggaFlgSJzazt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare finetuning dataset**"
      ],
      "metadata": {
        "id": "Ut1HVvGgO4NJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_FILE_PATH = os.path.join(DATASET_DIR, selected_dataset)\n",
        "train_ds, val_ds, _ = prepare_training_and_test_data(dataset_file=DATASET_FILE_PATH, dataset_type=DATASET_TYPE)\n",
        "\n",
        "print(\"Dataset type ......... \", DATASET_TYPE)\n",
        "print(\"Training dataset ..... \", len(train_ds))\n",
        "print(\"Validation dataset ... \", len(val_ds))"
      ],
      "metadata": {
        "id": "kX7sud64GhsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load pre-trained SentenceTransformer and Finetune and then save it**"
      ],
      "metadata": {
        "id": "TBNqdYQgO8VV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "print(\"Loading pre-trained Sentence Transformer\")\n",
        "model = load_pretrained_similarity_model(model_dir=PRETRAINED_MODEL_DIR, model_name=MODEL_NAME)\n",
        "\n",
        "print(\"Finetuning Sentence Transformer\")\n",
        "finetune_similarity_model(model, PRETRAINED_MODEL_DIR, train_ds, val_ds, epochs=1)\n",
        "\n",
        "print(\"Saving finetuned Sentence Transfomer under \", FINETUNED_MODEL_DIR)\n",
        "save_finetuned_similarity_model(model, FINETUNED_MODEL_DIR)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3L7ZuSF-HOIT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "abbad02737ef4cf58550b5f70107e0d7",
            "ee84ae89dcfa4cbbb70b745dae617c17",
            "501068b7ecf74c76a2992336ff7cdaf9",
            "6daf6a39a4984848bbbd2695fa934832",
            "5957912e7ec6478f8faa8a08adce329a",
            "c016719a86d648dd801b97b5de63040a",
            "8feddd25a5784e9cbc82cd4f816ef9c3",
            "d697b98e22e341c688c6310b9380fde7",
            "94d838dd9bf44445a572499a005dd14b",
            "298ae072e73340cb93b4921473ed463f",
            "df85c74c76eb458f8fc5731c3398df20"
          ]
        },
        "outputId": "302a2496-b8bc-49aa-ab94-e9b9671ceeaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pre-trained Sentence Transformer\n",
            "Loading SentenceTransformer('all-MiniLM-L6-v2')\n",
            "Finetuning Sentence Transformer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
            "/usr/local/lib/python3.11/dist-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
            "  table = cls._concat_blocks(blocks, axis=0)\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abbad02737ef4cf58550b5f70107e0d7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20215' max='20215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20215/20215 15:33, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Val-eval Pearson Cosine</th>\n",
              "      <th>Val-eval Spearman Cosine</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.164800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.145300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.136700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.134800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.131700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.128500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.126800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.124500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.119300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.123700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.123400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.118400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.120400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.118200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.117500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.119600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.114100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.113400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.114900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.116200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.116800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.115200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.113200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.113400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.112400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.112400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.110400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.111300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.107200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.112600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.109500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.113700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.111400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.107700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.111800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.109900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20215</td>\n",
              "      <td>0.109900</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.736666</td>\n",
              "      <td>0.719064</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving finetuned Sentence Transfomer under  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-similarity-recommendation/similarity-quora/model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build VectorDB for future similarity searches and save it**"
      ],
      "metadata": {
        "id": "GWYFbnppPHFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building Similarity Vector Database\")\n",
        "vdb = build_similarity_database(model, train_ds + val_ds)\n",
        "\n",
        "print(\"Saving Similarity Vector Database to \", VECTORDB_FILE)\n",
        "save_similarity_database(vdb, VECTORDB_FILE)"
      ],
      "metadata": {
        "id": "QvMQKSeTImyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "c98b24717640440084f64ae0cfa35f1b",
            "9ad3534d90b5440a938836fadaf838b5",
            "f4ee372bb4c34c6eaf5f5eae8685f40f",
            "23572b6e73c64bf58f8c8316698e696b",
            "d40028994b7d4d47bf6f299fd819027d",
            "4e7c92c5519e47f99153984dc2dbcd03",
            "f476b9917c314170816226c42a263c76",
            "f821a4f38dfc4d148e734c3c72626c47",
            "a2c33b7a6960498fa2f0bbe4753631ca",
            "b87ec200cc4b41f88c181fc7a19fba94",
            "bf4bd943a2264b8daf2f6f139b37cf3a"
          ]
        },
        "outputId": "0a52430a-243b-4c87-a44c-eabf64ee0212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Similarity Vector Database\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/15433 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c98b24717640440084f64ae0cfa35f1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Similarity Vector Database to  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-similarity-recommendation/similarity-quora/vectordb/vdb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Running Inferencing**\n",
        "**Using our Finetuned SentenceTransformer LLM and Similarity VectorDB**"
      ],
      "metadata": {
        "id": "EqnWMVZyPWS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_finetuned_similarity_model(FINETUNED_MODEL_DIR)\n",
        "vdb = load_similarity_database(VECTORDB_FILE)"
      ],
      "metadata": {
        "id": "BUH5mjjXPK3y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a9e2b8-114f-41d5-97f7-8010acbfe4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading finetuned model from /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-similarity-recommendation/similarity-quora/model\n",
            "Loading Similarity VectorDB from  /content/drive/MyDrive/cidl/models/finetuned/bert/s3e2-similarity-recommendation/similarity-quora/vectordb/vdb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_FILE_PATH = os.path.join(DATASET_DIR, selected_dataset)\n",
        "_, _, test_ds = prepare_training_and_test_data(dataset_file=DATASET_FILE_PATH, dataset_type=DATASET_TYPE)"
      ],
      "metadata": {
        "id": "q5tVFEimJzBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c77633e-d6cc-4cb8-ccad-08db68d8689e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset from file: /content/drive/MyDrive/cidl/datasets/similarity-quora.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find similar texts**"
      ],
      "metadata": {
        "id": "aaEmDqkqPl_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for row in test_ds[:10]:\n",
        "    sentence = row.texts[0]\n",
        "    sims = find_similar(model, vdb, sentence, topk=10)\n",
        "    print(f\"\\nQuery: {sentence}\")\n",
        "    for text, score in sims:\n",
        "        pscore = score * 100\n",
        "        print(f\"  → {text[:80]}  [{pscore:.1f}%]\")"
      ],
      "metadata": {
        "id": "iN8kZ53eM5ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d99cfe7-8388-498f-be38-eb36c018b7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Query: How do I play Pokémon GO in Korea?\n",
            "  → How should I celebrate my birthday in Korea?  [87.1%]\n",
            "  → How can I self-publish a book in Korea?  [83.0%]\n",
            "  → Are there any good incubators or accelerators in Korea?  [80.6%]\n",
            "  → What factors are stifling the startup entrepreneurship ecosystem in Korea?  [77.1%]\n",
            "  → Are there community colleges in Korea?  [76.5%]\n",
            "  → What are the best mobile ad networks in Korea?  [75.7%]\n",
            "  → What is the longest roller coaster in the United Kingdom and how does it compare  [73.7%]\n",
            "  → What is a good company to invest in in Korea?  [73.6%]\n",
            "  → Is there any Socket Server made in Korea?  [72.9%]\n",
            "\n",
            "Query: What are some of the best side dishes for crab cakes?\n",
            "  → What are the best side dishes to serve with crab cakes for dinner?  [96.4%]\n",
            "  → How long do crab cakes last in the fridge? Is there something I could do to make  [90.9%]\n",
            "  → What are some great side dishes to serve with crab salad?  [88.7%]\n",
            "  → What are some good side dishes to serve with crab legs?  [87.8%]\n",
            "  → What are the best recipes for crab stuffed salmon?  [80.8%]\n",
            "  → How can I make easy crab cakes?  [79.8%]\n",
            "  → How long do you boil crab legs? What is the best boiling temperature to use?  [71.1%]\n",
            "\n",
            "Query: Which is more advisable and better material for a crash test in automobiles, ductile or brittle?\n",
            "\n",
            "Query: How do I improve logical programming skills?\n",
            "  → How can one improve his logical reasoning abilities?  [81.3%]\n",
            "  → How can we improve our logical ability?  [81.2%]\n",
            "  → How do I improve my logical thinking abilities?  [80.5%]\n",
            "  → How do I solve logical problems?  [80.3%]\n",
            "  → What are the effective ways to improve our logical thinking?  [79.3%]\n",
            "  → What is the best way to develop logical thinking?  [78.8%]\n",
            "  → What is the best way to improve your logical reasoning?  [78.5%]\n",
            "  → How do I improve logic building skills in a Programming language?  [78.4%]\n",
            "  → How do I improve my logical thinking and problem solving skills?  [74.2%]\n",
            "  → How can an adult get better at logical thinking?  [71.3%]\n",
            "\n",
            "Query: How close we are to see 3rd world war?\n",
            "  → How close are we (or not close) right now to World War 3?  [95.3%]\n",
            "  → Will there be a third World War?  [94.2%]\n",
            "  → Is a third World War imminent?  [93.9%]\n",
            "  → Do you see a possibility for a third world war?  [93.8%]\n",
            "  → How close are we to World War Three, and how bad would it be?  [93.6%]\n",
            "  → Is there a chance for world war 3 ?  [93.4%]\n",
            "  → Do you think that there will be a Third World War?  [93.2%]\n",
            "  → Are we getting closer to world war 3?  [93.2%]\n",
            "  → Is a third world war coming?  [93.1%]\n",
            "\n",
            "Query: What do Chinese people think about Donald Trump?\n",
            "  → What do Chinese people think of Donald Trump?  [99.6%]\n",
            "  → What do Chinese people think of Donald Trump's victory?  [97.9%]\n",
            "  → What do Chinese think of Donald Trump?  [97.8%]\n",
            "  → What do Chinese think of Donald Trump as a leader?  [94.0%]\n",
            "  → What do Chinese people think of Rodrigo Duterte?  [86.5%]\n",
            "  → What do Chinese people think about Narendra Modi?  [81.9%]\n",
            "  → What do Chinese people think of Narendra Modi?  [80.3%]\n",
            "  → What is the biggest problem with Chinese on Quora?  [71.3%]\n",
            "  → What will Hillary Clinton's Chinese policy be, if she is elected?  [70.7%]\n",
            "\n",
            "Query: How many hours a week do Google employees work?\n",
            "  → How many hours a day do Google employees work on average?  [95.1%]\n",
            "  → How much do Google employees get paid?  [78.3%]\n",
            "  → What is the management style at Google?  [76.2%]\n",
            "  → How much money does Google make per hour?  [75.4%]\n",
            "  → How much do product managers make at Google?  [75.3%]\n",
            "  → What are the salary ranges of each level in Google's technical career track?  [73.4%]\n",
            "  → What is Google's business model?  [71.6%]\n",
            "  → How much does Google earn per day?  [70.8%]\n",
            "  → Is there a phone number at Google's headquarters that actually has someone answe  [70.5%]\n",
            "  → What is the phone number to contact Google customer care department?  [69.9%]\n",
            "\n",
            "Query: How can we follow a Quora question privately without the knowledge of others users?\n",
            "  → Is it possible to send messages privately through Quora to someone answering my   [78.5%]\n",
            "  → How do you ask a question in Quora with anonymity on?  [77.7%]\n",
            "  → How should I put question anonymously on Quora?  [76.5%]\n",
            "  → How do I answer questions anonymously on Quora?  [76.5%]\n",
            "  → How can I add question anonymously on Quora?  [75.7%]\n",
            "  → How do I answer a question anonymously in Quora?  [73.9%]\n",
            "  → How do I ask question on Quora anonymously?  [73.2%]\n",
            "  → How does one go about posting anonymously on Quora?  [72.8%]\n",
            "  → How ask anonymous question on Quora?  [72.6%]\n",
            "  → I am new to Quora. How can I answer a question anonymously?  [71.7%]\n",
            "\n",
            "Query: Why are cats so overprotective?\n",
            "  → Are today's parents overprotective parents?  [61.1%]\n",
            "  → Why is my cat so protective?  [60.7%]\n",
            "\n",
            "Query: How do I improve logical programming skills?\n",
            "  → How can one improve his logical reasoning abilities?  [81.3%]\n",
            "  → How can we improve our logical ability?  [81.2%]\n",
            "  → How do I improve my logical thinking abilities?  [80.5%]\n",
            "  → How do I solve logical problems?  [80.3%]\n",
            "  → What are the effective ways to improve our logical thinking?  [79.3%]\n",
            "  → What is the best way to develop logical thinking?  [78.8%]\n",
            "  → What is the best way to improve your logical reasoning?  [78.5%]\n",
            "  → How do I improve logic building skills in a Programming language?  [78.4%]\n",
            "  → How do I improve my logical thinking and problem solving skills?  [74.2%]\n",
            "  → How can an adult get better at logical thinking?  [71.3%]\n"
          ]
        }
      ]
    }
  ]
}
